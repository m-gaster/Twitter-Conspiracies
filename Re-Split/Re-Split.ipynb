{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import twint\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# from datetime import timedelta, date\n",
    "# from collections import Counter\n",
    "# from ast import literal_eval\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXISTING_LOOKUPS_PATHS_ROOT = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/All Conspiracy Tweeters' Tweets/Split\"    # folder which contains folders which contain CT account tweets\n",
    "EXISTING_LOOKUPS_PATHS = [ EXISTING_LOOKUPS_PATHS_ROOT + f\" {split}/\" for split in [5,6,7,8,9] ]      # list of folders which contain CT account tweets\n",
    "\n",
    "NOTEBOOK_TEMPLATE_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Re-Split Lookup/Tweet Lookup TEMPLATE.ipynb\"    # path containing .json format of jupyter file template for the lookup\n",
    "\n",
    "NEW_LOOKUPS_PATH_ROOT = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/New Split Tweet Lookups/\"    # folder which contains folder for each split\n",
    "\n",
    "NEW_SPLIT_LIST_SAVE_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Getting Conspiracy Hashtag Users/New Splits/\"    # folder containing all N splits of the master user list\n",
    "\n",
    "NEW_NOTEBOOKS_PATH_ROOT = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/New Split Scripts/\"    # where to save the new .ipynb notebooks\n",
    "\n",
    "MASTER_USER_LIST_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/MASTER LIST - All CT Link and Hashtag Users/All CT Link and Hashtag Users.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SPLITS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new folders to put splits in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id_split_save_paths = []\n",
    "\n",
    "for split in range(0, NUM_SPLITS):\n",
    "    \n",
    "    newpath = fr'{NEW_LOOKUPS_PATH_ROOT}Split {split}' \n",
    "    \n",
    "    all_id_split_save_paths.append(newpath)\n",
    "    \n",
    "    # make a new folder\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Split User ID List\n",
    "\n",
    "1. Find users who have been looked up already\n",
    "    * Mine\n",
    "    * Luke's\n",
    "2. Bring that information into the master list of users\n",
    "3. Use ARC to find all users in our master list with GeoCov19 locations\n",
    "    * Do this so we can search them first\n",
    "4. Split the users into mine and luke's (only returning the users who haven't been searched up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all users who have been searched\n",
    "\n",
    "Luke needs to run the following 5 cells and send me the output (the users he's already searched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_searched_ids(path):\n",
    "    \n",
    "    return [int(f.split('_')[0]) for f in listdir(path) if isfile(join(path, f))]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_searched_users(split_list):\n",
    "    '''\n",
    "    input = list of splits (ints) detailing which split of the dataset we're recovering (e.g. I'm doing [5,6,7,8,9])\n",
    "    output = np.array of all users (hashtag and link) who have been searched for\n",
    "    '''   \n",
    "    \n",
    "    #create a list of all users we've searched for\n",
    "    all_searched_users = []\n",
    "    \n",
    "    for split in split_list:\n",
    "\n",
    "        split_n_path = EXISTING_LOOKUPS_PATHS_ROOT + \" \" + str(split)\n",
    "\n",
    "        searched_users = recover_searched_ids(split_n_path)\n",
    "        \n",
    "        all_searched_users += searched_users\n",
    "        \n",
    "    return np.array(all_searched_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_search_updated_dataset(split_list):\n",
    "    '''\n",
    "    input = list of splits (ints) detailing which split of the dataset we're recovering (e.g. I'm doing [5,6,7,8,9])\n",
    "    output = dataframe of all users (hashtag and link) which is updated with searched account['Searched'] == 1\n",
    "    '''   \n",
    "    all_searched_users = find_searched_users(split_list)\n",
    "        \n",
    "    # bring list of searched users into master list\n",
    "    all_users = pd.read_csv(MASTER_USER_LIST_PATH).set_index('ID')\n",
    "    \n",
    "    if 'Searched' not in all_users.columns:\n",
    "        all_users['Searched'] = 0\n",
    "        \n",
    "    if 'Error' not in all_users.columns:\n",
    "        all_users['Error'] = 0\n",
    "    \n",
    "    all_users['Searched'].loc[all_searched_users] = 1\n",
    "\n",
    "    \n",
    "    return all_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASTER_USER_LIST = recreate_search_updated_dataset([5,6,7,8,9])\n",
    "\n",
    "# MASTER_USER_LIST[['Num CT Tweets - HT', 'Num CT Tweets - LINK']] = MASTER_USER_LIST[['Num CT Tweets - HT', 'Num CT Tweets - LINK']].fillna(0)\n",
    "\n",
    "# MASTER_USER_LIST.sort_values(by=['Num CT Tweets - HT', 'Num CT Tweets - LINK'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "mikhael_searched_users = find_searched_users([5,6,7,8,9])\n",
    "lukes_searched_users = pd.read_csv(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Re-Split Lookup/users luke has looked up.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SEARCHED_USERS = np.concatenate([lukes_searched_users['0'].to_numpy(), mikhael_searched_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "ALL_CT_USERS = pd.read_csv(MASTER_USER_LIST_PATH)\n",
    "\n",
    "ALL_CT_USERS['Searched'] = 0\n",
    "\n",
    "ALL_CT_USERS['Error'] = 0\n",
    "\n",
    "ALL_CT_USERS.set_index('ID', inplace=True)\n",
    "\n",
    "ALL_CT_USERS.rename(columns={'Unnamed: 0': 'Original Order'})\n",
    "\n",
    "ALL_CT_USERS['Searched'].loc[ALL_SEARCHED_USERS] = 1\n",
    "\n",
    "ALL_CT_USERS[['Num CT Tweets - HT', 'Num CT Tweets - LINK']] = ALL_CT_USERS[['Num CT Tweets - HT', 'Num CT Tweets - LINK']].fillna(0)\n",
    "\n",
    "# ALL_CT_USERS.sort_values(by=['Num CT Tweets - HT', 'Num CT Tweets - LINK'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weighted score to sort CT users by\n",
    "ALL_CT_USERS['Weighted Num Tweets'] = ALL_CT_USERS['Num CT Tweets - HT']/ALL_CT_USERS['Num CT Tweets - HT'].sum()\n",
    "ALL_CT_USERS['Weighted Num Links'] = ALL_CT_USERS['Num CT Tweets - LINK']/ALL_CT_USERS['Num CT Tweets - LINK'].sum()\n",
    "ALL_CT_USERS['Weighted CT Score'] = ALL_CT_USERS['Weighted Num Links'] + ALL_CT_USERS['Weighted Num Tweets']\n",
    "\n",
    "ALL_CT_USERS.sort_values(['Weighted CT Score'], ascending=False, inplace=True)\n",
    "\n",
    "ALL_CT_USERS.drop(['Weighted CT Score', 'Weighted Num Tweets', 'Weighted Num Links'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_search = ALL_CT_USERS[ ALL_CT_USERS['Searched']==0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "luke_users_to_search = users_to_search[0::2]\n",
    "mikhael_users_to_search = users_to_search[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "lukes_splits = {split:luke_users_to_search[split::NUM_SPLITS] for split in range(0, NUM_SPLITS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "mikhaels_splits = {split:mikhael_users_to_search[split::NUM_SPLITS] for split in range(0, NUM_SPLITS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in range(0, NUM_SPLITS):\n",
    "    mikhaels_splits[split].to_csv(NEW_SPLIT_LIST_SAVE_PATH + f\"Split {split}.csv\")\n",
    "\n",
    "LUKES_SPLIT_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Getting Conspiracy Hashtag Users/New Splits/New Splits for Luke/\"\n",
    "for split in range(0, NUM_SPLITS):\n",
    "    lukes_splits[split].to_csv(LUKES_SPLIT_PATH + f\"Split {split}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out new split of users (links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_user_df = pd.read_csv(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Getting Conspiracy Link Users/All Link Users.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_user_ids = link_user_df['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_user_ids = np.array_split(link_user_ids, 2)\n",
    "\n",
    "lukes_link_users = link_user_ids[0]\n",
    "mikhaels_link_users = link_user_ids[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting old and new splits together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "luke_final_users = list(lukes_link_users) + list(lukes_og_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "mikhaels_final_users = list(mikhaels_link_users) + list(mikhaels_og_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_split_n_notebook(split):\n",
    "    \n",
    "    save_ipynb_path = NEW_NOTEBOOKS_PATH_ROOT + f'Split {split}.ipynb'\n",
    "    \n",
    "    save_path_for_this_splits_users = NEW_LOOKUPS_PATH_ROOT + f'Split {split}'\n",
    "    \n",
    "    save_path_to_update_searched_users = NEW_SPLIT_LIST_SAVE_PATH + f'Split {split}.csv'\n",
    "    \n",
    "    with open(NOTEBOOK_TEMPLATE_PATH, 'r') as fp:\n",
    "        notebook = json.load(fp)\n",
    "    \n",
    "    for cell in notebook['cells']:\n",
    "        if cell['cell_type']=='code':\n",
    "            cell['source'] = [x.replace('REPLACE_SPLIT_USER_IDS_PATH', \"'\" + save_path_to_update_searched_users + \"'\").replace('REPLACE_PATH_ROOT', \"'\" + save_path_for_this_splits_users + \"'\").replace('REPLACE_WITH_SPLIT_NUMBER', str(split)) for x in cell['source']]\n",
    "            \n",
    "    with open(save_ipynb_path, 'w') as fp:\n",
    "        json.dump(notebook, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for split in range(0, NUM_SPLITS):\n",
    "    gen_split_n_notebook(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_split_n_notebook(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
