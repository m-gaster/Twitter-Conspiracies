{"cells": [{"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import time\n", "import os\n", "from os import listdir\n", "from os.path import isfile, join\n", "from ast import literal_eval\n", "import re\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/html": ["<style>.container { width:97.5% !important; }</style>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from IPython.core.display import display, HTML\n", "display(HTML(\"<style>.container { width:97.5% !important; }</style>\"))\n", "\n", "pd.set_option('display.max_colwidth', None)\n", "pd.set_option('display.max_rows', 500)\n", "pd.set_option('display.max_columns', 500)\n", "pd.set_option('display.width', 1000)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# What this file does:\n", "\n", "1. Label each tweet as a CT tweet or not (Naive approach)\n", "    * Contains Hashtags?\n", "    * Contains Links?\n", "2. Clean Tweets for BERT\n", "    * Strings are converted to lists of words\n", "    * ['CLS'] at the beginning, ['SEP'] at the end\n", "    * All CT hashtags are removed\n", "    * All NON-CT hashtags have the pound sign removed\n", "    * All '@' symbols are removed\n", "3. Save a new .json file for BERT input\n", "    * File has two columns:\n", "        * Cleaned Tweet\n", "        * CT Label (1:CT, 0:Not CT)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Hashtag and Link Lists"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"code_folding": []}, "outputs": [], "source": ["general_conspiracy_hashtags = [\n", "    'plandemic',\n", "    'scamdemic',\n", "    'covidhoax',\n", "    'nwo',\n", "    'covid1984',\n", "    'plandemia',\n", "    'agenda21',\n", "    'thegreatreset',\n", "    'agenda2030',\n", "    'newworldorder',\n", "    'wakeupamerica',\n", "#     'wakeup',\n", "    'openamericanow',\n", "    'firefauci',\n", "    'wwg1wga',\n", "    'qanon',\n", "    'coronahoax'\n", "]\n", "\n", "keywords = [\n", "    'plandemic',\n", "    'scamdemic',\n", "    'covidhoax',\n", "    'covid hoax',\n", "    'covid1984',\n", "    'plandemia',\n", "    'new world order',\n", "    'wake up america',\n", "    'open america now',\n", "    'fire fauci',\n", "    'wwg1wga',\n", "    'qanon',\n", "    'coronahoax',\n", "    'corona hoax',\n", "]\n", "\n", "CT_link_list = ['zerohedge.com', 'infowars.com', 'principia-scientific.com',\n", "'tx.voice-truth.com', 'humansarefree.com', 'activistpost.com'\n", "'gnews.org', 'wakingtimes.com', 'brighteon.com', 'thewallwillfall.org', 'sott.net',]\n", "\n", "\n", "hashtag_set = set(['#' + tag for tag in general_conspiracy_hashtags])\n", "keyword_set = set(keywords)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["covid_conspiracy_hashtags = [\n", "    'plandemic',\n", "    'scamdemic',\n", "    'covidhoax',\n", "    'covid1984',\n", "    'plandemia',\n", "    'firefauci',\n", "    'coronahoax'\n", "]\n", "\n", "covid_keywords = [\n", "    'plandemic',\n", "    'scamdemic',\n", "    'covidhoax',\n", "    'covid hoax',\n", "    'covid1984',\n", "    'plandemia',\n", "    'fire fauci',\n", "    'coronahoax',\n", "    'corona hoax',\n", "]\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["NON_covid_conspiracy_hashtags = list( set(general_conspiracy_hashtags) - set(covid_conspiracy_hashtags) )\n", "\n", "NON_covid_keywords = list( set(keywords) - set(covid_keywords) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Columns to aggregate"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["link_tag_keyword_cols = []\n", "\n", "for link in CT_link_list:\n", "    link_tag_keyword_cols.append(f'Link - {link}')      \n", "for hashtag in general_conspiracy_hashtags:\n", "    link_tag_keyword_cols.append(f'Tag - {hashtag}')\n", "for keyword in keywords:\n", "    link_tag_keyword_cols.append(f'Keyword - {keyword}')\n", "\n", "# COVID_link_tag_keyword_cols = []\n", "COVID_tag_keyword_cols = []\n", "\n", "# for link in CT_link_list:\n", "#     COVID_link_tag_keyword_cols.append(f'Link - {link}')      \n", "for hashtag in covid_conspiracy_hashtags:\n", "    COVID_tag_keyword_cols.append(f'Tag - {hashtag}')\n", "for keyword in covid_keywords:\n", "    COVID_tag_keyword_cols.append(f'Keyword - {keyword}')\n", "\n", "    \n", "    \n", "# IF I WANT DUMMIES BROKEN DOWN BY ('POST', 'RETWEET', 'TOTAL')!!!!!\n", "# for link in CT_link_list:\n", "#     for tweet_type in ('POST', 'RETWEET', 'TOTAL'):\n", "#         link_tag_keyword_cols.append(f'Link - {link} - {tweet_type}')      \n", "# for hashtag in general_conspiracy_hashtags:\n", "#     for tweet_type in ('POST', 'RETWEET', 'TOTAL'):\n", "#         link_tag_keyword_cols.append(f'Tag - {hashtag} - {tweet_type}')\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["def hashtag_in_list(list_of_hashtags_in_tweet):\n", "    return any(hashtag.upper() in [tag.upper() for tag in list_of_hashtags_in_tweet] for hashtag in general_conspiracy_hashtags)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clean Tweet Function"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["def clean_tweet(tweet):\n", "    '''\n", "    input = tweet (str)\n", "    output = cleaned_tweet(str)\n", "    '''\n", "    \n", "#     return [['CLS']] + [x.replace('#','') for x in tweet.split() if not (x.startswith(('http','@')) or x in keyword_set or x in hashtag_set)] + [['SEP']]\n", "    try:\n", "        return ' '.join([x.replace('#','') for x in tweet.split() if not (x.startswith(('http','@')) or x in keyword_set or x in hashtag_set)])\n", "    except:\n", "        return ''"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Final Function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["list_of_non_english_users = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["relevant_columns = [\n", "    'date',\n", "    'hashtags',\n", "    'urls',\n", "    'tweet',\n", "    'language'\n", "]\n", "\n", "datatypes = {'urls':'str', 'tweet':'str', 'language':'float64'}\n", "\n", "re_escape_keywords = '|'.join([re.escape(word) for word in keywords])\n", "re_escape_links = '|'.join([re.escape(link) for link in CT_link_list])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "# def label_CT_tweets(df):\n", "    \n", "#     return ( df['tweet'].str.contains(re_escape_keywords, case=False) | df['urls'].str.contains(re_escape_links, case=False) | df['hashtags'].apply(hashtag_in_list)).astype(int)\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"code_folding": []}, "outputs": [], "source": ["def add_CT_dummies(READ_PATH_ROOT, DF_SAVE_PATH_ROOT, BERT_GENERAL_CT_INPUT_SAVE_PATH_ROOT, BERT_COVID_CT_INPUT_SAVE_PATH_ROOT, BERT_NON_COVID_CT_INPUT_SAVE_PATH_ROOT, USER):\n", "    \n", "    READ_PATH = READ_PATH_ROOT + USER + '_TWEETS.csv'\n", "    DF_SAVE_PATH = DF_SAVE_PATH_ROOT + USER + '_CLEANED_TWEETS.csv'\n", "    BERT_GENERAL_CT_INPUT_SAVE_PATH = BERT_GENERAL_CT_INPUT_SAVE_PATH_ROOT + USER + 'GEN_CT_BERT_INPUT.csv'\n", "    BERT_COVID_CT_INPUT_SAVE_PATH = BERT_COVID_CT_INPUT_SAVE_PATH_ROOT + USER + 'COV_CT_BERT_INPUT.csv'\n", "    BERT_NON_COVID_CT_INPUT_SAVE_PATH = BERT_NON_COVID_CT_INPUT_SAVE_PATH_ROOT + USER + 'NON_COV_CT_BERT_INPUT.csv'\n", "    \n", "    df = pd.read_csv(READ_PATH, parse_dates=['date'], usecols=relevant_columns, converters={'hashtags': eval}, dtype=datatypes)#.set_index('retweet')\n", "    \n", "#     print(df[ df['tweet'].isnull() ]['tweet'])\n", "    \n", "#     df.query('language == 1', inplace=True)\n", "\n", "    df.insert(loc=1, column='Cleaned Tweet', value=df['tweet'].apply(clean_tweet))\n", "    \n", "    df['tweet'] = df['tweet'].fillna('')\n", "    \n", "    df = df[df['tweet'].str.split().map(len) > 2] #only keep tweets with more than 2 words\n", "\n", "#     df = df[ df['Cleaned Tweet'].map(len) > 4 ] #only keep tweets with more than 2 words (cleaned tweets start and end with tokens)\n", "\n", "#         del df['Unnamed: 0']\n", "\n", "#     df['hashtags'] = df['hashtags'].apply(literal_eval)\n", "\n", "    # create dummies for each hashtag and link (in each tweet)    \n", "    for link in CT_link_list:\n", "#         df[f'Link - {link}'] = df['urls'].str.contains(link, case=False).astype(int)\n", "        df[f'Link - {link}'] = df['urls'].str.contains(re.escape(link), case=False).astype(int)\n", "\n", "    for tag in general_conspiracy_hashtags:\n", "#         df[f'Tag - {tag}'] = df['hashtags'].apply(hashtag_in_list).astype(int)\n", "        df[f'Tag - {tag}'] = df['hashtags'].apply(hashtag_in_list).astype(int)\n", "\n", "    for keyword in keywords:\n", "#         df[f'Keyword - {keyword}'] = df['tweet'].str.contains(keyword, case=False).astype(int)\n", "        df[f'Keyword - {keyword}'] = df['tweet'].str.contains(re.escape(keyword), case=False).astype(int)\n", "\n", "    df.insert(loc=0, column='CT Tweet (Dummy)', value=(df[link_tag_keyword_cols].sum(axis=1) > 0).astype(int))\n", "    df.insert(loc=0, column='COVID-SPECIFIC CT Tweet (Dummy)', value=(df[COVID_tag_keyword_cols].sum(axis=1) > 0).astype(int))\n", "    df.insert(loc=2, column='NON-COVID CT Tweet (Dummy)', value=(df['CT Tweet (Dummy)'] - df['COVID-SPECIFIC CT Tweet (Dummy)']).astype(int))\n", "\n", "\n", "#     somewhat unrelated - fix broken 'retweet' boolean column\n", "    df['retweet'] = df['tweet'].str.startswith('RT').astype(int)\n", "\n", "#     return df\n", "\n", "    df.to_csv(DF_SAVE_PATH, index=False)\n", "\n", "    df[(df['language']==1) & (df['CT Tweet (Dummy)']==1)][['Cleaned Tweet', 'CT Tweet (Dummy)']].rename(columns={'Cleaned Tweet':'sentence1', 'CT Tweet (Dummy)':'label'}).to_csv(BERT_GENERAL_CT_INPUT_SAVE_PATH, index=False)\n", "    \n", "    df[(df['language']==1) & (df['COVID-SPECIFIC CT Tweet (Dummy)']==1)][['Cleaned Tweet', 'COVID-SPECIFIC CT Tweet (Dummy)']].rename(columns={'Cleaned Tweet':'sentence1', 'COVID-SPECIFIC CT Tweet (Dummy)':'label'}).to_csv(BERT_COVID_CT_INPUT_SAVE_PATH, index=False)\n", "    \n", "    df[(df['language']==1) & (df['NON-COVID CT Tweet (Dummy)']==1)][['Cleaned Tweet', 'NON-COVID CT Tweet (Dummy)']].rename(columns={'Cleaned Tweet':'sentence1', 'NON-COVID CT Tweet (Dummy)':'label'}).to_csv(BERT_NON_COVID_CT_INPUT_SAVE_PATH, index=False)\n", "\n", "#         return df[['Cleaned Tweet', 'CT Tweet (Dummy)']].rename(columns={'Cleaned Tweet':'sentence1', 'CT Tweet (Dummy)':'label'})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# TEST_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/New Split Tweet Lookups/Split 10/3254815086_TWEETS.csv\"\n", "\n", "# TEST_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/New Split Tweet Lookups/Split 10/\"\n", "# USER = \"570927672\"\n", "\n", "# test = pd.read_csv(TEST_PATH)\n", "\n", "# %%time \n", "\n", "# fnc_test = add_CT_dummies(READ_PATH_ROOT=TEST_PATH_ROOT, DF_SAVE_PATH_ROOT=\"0\", BERT_INPUT_SAVE_PATH_ROOT=\"0\", USER=USER)\n", "\n", "# fnc_test[fnc_test['CT Tweet (Dummy)']==1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Path Root to Create .CSVs for BERT Input\n", "\n", "sentence1, label"]}, {"cell_type": "code", "execution_count": null, "metadata": {"code_folding": []}, "outputs": [], "source": ["# BERT_CSV_folder_path = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/BERT/BERT Input CSVs/\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Apply Final Function"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## New ARC Version"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ARC_TWEETS_READ_PATH_ROOT = r\"/arc/project/st-tlemieux-1/data/Twint-Data/Mikhael/New-Splits/Split 8/\" #SPECIFY SUB-FOLDER \n", "# e.g. /arc/project/st-tlemieux-1/data/Twint-Data/Luke/New-Splits/Split <X>\" for X=[0:19]\n", "\n", "ARC_CLEANED_WRITE_PATH_ROOT = r\"/scratch/st-tlemieux-1/lfrymire/cleaned_and_labeled_tweets/\" #SPECIFY SUB-FOLDER\n", "\n", "ARC_GEN_CT_BERT_INPUT_SAVE_PATH_ROOT = r\"/scratch/st-tlemieux-1/lfrymire/BERT-input/GEN-CT-Tweets/\"\n", "\n", "ARC_COV_CT_BERT_INPUT_SAVE_PATH_ROOT = r\"/scratch/st-tlemieux-1/lfrymire/BERT-input/COV-CT-Tweets/\"\n", "\n", "ARC_NON_COVID_CT_BERT_INPUT_SAVE_PATH_ROOT = r\"/scratch/st-tlemieux-1/lfrymire/BERT-input/NON-COV-CT-Tweets/\"\n", "\n", "\n", "\n", "USERS_WITH_TWEETS = [int(f.split('_')[0]) for f in listdir(ARC_TWEETS_READ_PATH_ROOT) if isfile(join(ARC_TWEETS_READ_PATH_ROOT, f))]\n", "\n", "for user in tqdm(USERS_WITH_TWEETS):\n", "    add_CT_dummies(READ_PATH_ROOT=ARC_TWEETS_READ_PATH_ROOT,\n", "                   DF_SAVE_PATH_ROOT=ARC_CLEANED_WRITE_PATH_ROOT,\n", "                   BERT_GENERAL_CT_INPUT_SAVE_PATH_ROOT=ARC_GEN_CT_BERT_INPUT_SAVE_PATH_ROOT,\n", "                   BERT_COVID_CT_INPUT_SAVE_PATH_ROOT=ARC_COV_CT_BERT_INPUT_SAVE_PATH_ROOT,\n", "                   BERT_NON_COVID_CT_INPUT_SAVE_PATH_ROOT=ARC_NON_COVID_CT_BERT_INPUT_SAVE_PATH_ROOT,\n", "                   USER=str(user))\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Pre-Arc Version"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["# # folder containing folders of all split user lookups\n", "# OG_READ_PATH_ROOT = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/All Conspiracy Tweeters' Tweets/\"\n", "\n", "# # folder which will contain folders of all split user lookup AGGREGATIONS (weekly CT activity)\n", "# OG_WRITE_PATH_ROOT = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/Labeled OG Tweets/\"\n", "\n", "# og_splits = [5,6,7,8,9]\n", "\n", "# for split in og_splits:\n", "#     local_read_path_root = OG_READ_PATH_ROOT + f'Split {split}/'\n", "#     local_write_path_root = OG_WRITE_PATH_ROOT + f'Split {split}/'\n", "    \n", "#     # make new folders to store processed tweets\n", "#     if not os.path.exists(local_write_path_root):\n", "#         os.makedirs(local_write_path_root)\n", "    \n", "#     # find all users who have been searched and stored in this folder\n", "#     users_in_this_split = [int(f.split('_')[0]) for f in listdir(local_read_path_root) if isfile(join(local_read_path_root, f))]\n", "\n", "#     # aggregate each user's tweets\n", "#     for user in users_in_this_split:\n", "#         print(user)\n", "#         add_CT_dummies(READ_PATH_ROOT=local_read_path_root, DF_SAVE_PATH_ROOT=local_write_path_root, BERT_INPUT_SAVE_PATH_ROOT=BERT_CSV_folder_path, USER=str(user))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}, "latex_envs": {"LaTeX_envs_menu_present": true, "autoclose": true, "autocomplete": true, "bibliofile": "biblio.bib", "cite_by": "apalike", "current_citInitial": 1, "eqLabelWithNumbers": true, "eqNumInitial": 1, "hotkeys": {"equation": "Ctrl-E", "itemize": "Ctrl-I"}, "labels_anchors": false, "latex_user_defs": false, "report_style_numbering": false, "user_envs_cfg": false}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 5}