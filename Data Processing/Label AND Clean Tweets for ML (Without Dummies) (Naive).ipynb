{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:97.5% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:97.5% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What this file does:\n",
    "\n",
    "1. Label each tweet as a CT tweet or not (Naive approach)\n",
    "    * Contains Hashtags?\n",
    "    * Contains Links?\n",
    "2. Clean Tweets for BERT\n",
    "    * Strings are converted to lists of words\n",
    "    * ['CLS'] at the beginning, ['SEP'] at the end\n",
    "    * All CT hashtags are removed\n",
    "    * All NON-CT hashtags have the pound sign removed\n",
    "    * All '@' symbols are removed\n",
    "3. Save a new .json file for BERT input\n",
    "    * File has two columns:\n",
    "        * Cleaned Tweet\n",
    "        * CT Label (1:CT, 0:Not CT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag and Link Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "general_conspiracy_hashtags = [\n",
    "    'plandemic',\n",
    "    'scamdemic',\n",
    "    'covidhoax',\n",
    "    'nwo',\n",
    "    'covid1984',\n",
    "    'plandemia',\n",
    "    'agenda21',\n",
    "    'thegreatreset',\n",
    "    'agenda2030',\n",
    "    'newworldorder',\n",
    "    'wakeupamerica',\n",
    "#     'wakeup',\n",
    "    'openamericanow',\n",
    "    'firefauci',\n",
    "    'wwg1wga',\n",
    "    'qanon',\n",
    "    'coronahoax'\n",
    "]\n",
    "\n",
    "keywords = [\n",
    "    'plandemic',\n",
    "    'scamdemic',\n",
    "    'covidhoax',\n",
    "    'covid hoax',\n",
    "    'covid1984',\n",
    "    'plandemia',\n",
    "    'new world order',\n",
    "    'wake up america',\n",
    "    'open america now',\n",
    "    'fire fauci',\n",
    "    'wwg1wga',\n",
    "    'qanon',\n",
    "    'coronahoax',\n",
    "    'corona hoax',\n",
    "]\n",
    "\n",
    "CT_link_list = ['zerohedge.com', 'infowars.com', 'principia-scientific.com',\n",
    "'tx.voice-truth.com', 'humansarefree.com', 'activistpost.com'\n",
    "'gnews.org', 'wakingtimes.com', 'brighteon.com','thewallwillfall.org','sott.net',]\n",
    "\n",
    "\n",
    "hashtag_set = set(['#' + tag for tag in general_conspiracy_hashtags])\n",
    "keyword_set = set(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns to aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_tag_keyword_cols = []\n",
    "\n",
    "for link in CT_link_list:\n",
    "    link_tag_keyword_cols.append(f'Link - {link}')      \n",
    "for hashtag in general_conspiracy_hashtags:\n",
    "    link_tag_keyword_cols.append(f'Tag - {hashtag}')\n",
    "for keyword in keywords:\n",
    "    link_tag_keyword_cols.append(f'Keyword - {keyword}')\n",
    "\n",
    "\n",
    "# IF I WANT DUMMIES BROKEN DOWN BY ('POST', 'RETWEET', 'TOTAL')!!!!!\n",
    "# for link in CT_link_list:\n",
    "#     for tweet_type in ('POST', 'RETWEET', 'TOTAL'):\n",
    "#         link_tag_keyword_cols.append(f'Link - {link} - {tweet_type}')      \n",
    "# for hashtag in general_conspiracy_hashtags:\n",
    "#     for tweet_type in ('POST', 'RETWEET', 'TOTAL'):\n",
    "#         link_tag_keyword_cols.append(f'Tag - {hashtag} - {tweet_type}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_in_list(list_of_hashtags_in_tweet):\n",
    "    return any(hashtag.upper() in [tag.upper() for tag in list_of_hashtags_in_tweet] for hashtag in general_conspiracy_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweet Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    input = tweet (str)\n",
    "    output = cleaned_tweet(str)\n",
    "    '''\n",
    "    \n",
    "    return [['CLS']] + [x.replace('#','') for x in tweet.split() if not (x.startswith(('http','@')) or x in keyword_set or x in hashtag_set)] + [['SEP']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_non_english_users = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_columns = [\n",
    "    'date',\n",
    "    'hashtags',\n",
    "    'urls',\n",
    "    'tweet',\n",
    "    'language'\n",
    "]\n",
    "\n",
    "datatypes = {'urls':'str', 'tweet':'str', 'language':'float64'}\n",
    "\n",
    "re_escape_keywords = '|'.join([re.escape(word) for word in keywords])\n",
    "re_escape_links = '|'.join([re.escape(link) for link in CT_link_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_CT_tweets(df):\n",
    "    \n",
    "    return ( df['tweet'].str.contains(re_escape_keywords, case=False) | df['urls'].str.contains(re_escape_links, case=False) | df['hashtags'].apply(hashtag_in_list)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "def label_CT_tweets(df):\n",
      "    \n",
      "    return ( df['tweet'].str.contains(re_escape_keywords, case=False) | df['urls'].str.contains(re_escape_links, case=False) | df['hashtags'].to_numpy().apply(hashtag_in_list)).astype(int)\n",
      "                                     ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\crackcocaine69xxx\\.ipython\\cython\\_cython_magic_9d5286ecb3440524a65faaf65cb9869c.pyx:3:38: undeclared name not builtin: re_escape_keywords\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "def label_CT_tweets(df):\n",
      "    \n",
      "    return ( df['tweet'].str.contains(re_escape_keywords, case=False) | df['urls'].str.contains(re_escape_links, case=False) | df['hashtags'].to_numpy().apply(hashtag_in_list)).astype(int)\n",
      "                                                                                               ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\crackcocaine69xxx\\.ipython\\cython\\_cython_magic_9d5286ecb3440524a65faaf65cb9869c.pyx:3:96: undeclared name not builtin: re_escape_links\n",
      "\n",
      "Error compiling Cython file:\n",
      "------------------------------------------------------------\n",
      "...\n",
      "def label_CT_tweets(df):\n",
      "    \n",
      "    return ( df['tweet'].str.contains(re_escape_keywords, case=False) | df['urls'].str.contains(re_escape_links, case=False) | df['hashtags'].to_numpy().apply(hashtag_in_list)).astype(int)\n",
      "                                                                                                                                                              ^\n",
      "------------------------------------------------------------\n",
      "\n",
      "C:\\Users\\crackcocaine69xxx\\.ipython\\cython\\_cython_magic_9d5286ecb3440524a65faaf65cb9869c.pyx:3:159: undeclared name not builtin: hashtag_in_list\n"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "def label_CT_tweets(df):\n",
    "    \n",
    "    return ( df['tweet'].str.contains(re_escape_keywords, case=False) | df['urls'].str.contains(re_escape_links, case=False) | df['hashtags'].apply(hashtag_in_list)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_CT_dummies(READ_PATH_ROOT, DF_SAVE_PATH_ROOT, BERT_INPUT_SAVE_PATH_ROOT, USER):\n",
    "    \n",
    "    READ_PATH = READ_PATH_ROOT + USER + '_TWEETS.csv'\n",
    "    DF_SAVE_PATH = DF_SAVE_PATH_ROOT + USER + '_CLEANED_TWEETS.csv'\n",
    "    BERT_INPUT_SAVE_PATH = BERT_INPUT_SAVE_PATH_ROOT + USER + '_BERT_INPUT.csv'\n",
    "    \n",
    "    df = pd.read_csv(READ_PATH, parse_dates=['date'], usecols=relevant_columns, converters={'hashtags': eval}, dtype=datatypes)#.set_index('retweet')\n",
    "    \n",
    "#     if df['language'].sum() / len(df) < .1: #if less than 10% of tweets are recognized as English    \n",
    "#         list_of_non_english_users.append(USER)\n",
    "    \n",
    "#     else:\n",
    "\n",
    "    df.query('language == 1', inplace=True)\n",
    "    \n",
    "    df = df[df['tweet'].str.split().map(len) > 2]\n",
    "\n",
    "    df.insert(loc=1, column='Cleaned Tweet', value=df['tweet'].apply(clean_tweet))\n",
    "\n",
    "#     df = df[ df['Cleaned Tweet'].map(len) > 4 ] #only keep tweets with more than 4 words\n",
    "\n",
    "#         del df['Unnamed: 0']\n",
    "\n",
    "#     df['hashtags'] = df['hashtags'].apply(literal_eval)\n",
    "\n",
    "    # create dummies for each hashtag and link (in each tweet)    \n",
    "#     for link in CT_link_list:\n",
    "#         df[f'Link - {link}'] = df['urls'].str.contains(link, case=False).astype(int)\n",
    "\n",
    "#     for tag in general_conspiracy_hashtags:\n",
    "#         df[f'Tag - {tag}'] = df['hashtags'].apply(hashtag_in_list).astype(int)\n",
    "\n",
    "#     for keyword in keywords:\n",
    "#         df[f'Keyword - {keyword}'] = df['tweet'].str.contains(keyword, case=False).astype(int)\n",
    "\n",
    "    df.insert(loc=0, column='CT Tweet (Dummy)', value=(label_CT_tweets(df)))\n",
    "\n",
    "\n",
    "#     somewhat unrelated - fix broken 'retweet' boolean column\n",
    "#     df['retweet'] = df['tweet'].str.startswith('RT').astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "    #df.to_csv(DF_SAVE_PATH, index=False)\n",
    "\n",
    "    #df[df['language']==1][['Cleaned Tweet', 'CT Tweet (Dummy)']].rename(columns={'Cleaned Tweet':'sentence1', 'CT Tweet (Dummy)':'label'}).to_csv(BERT_INPUT_SAVE_PATH, index=False)\n",
    "\n",
    "#         return df[['Cleaned Tweet', 'CT Tweet (Dummy)']].rename(columns={'Cleaned Tweet':'sentence1', 'CT Tweet (Dummy)':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/New Split Tweet Lookups/Split 10/3254815086_TWEETS.csv\"\n",
    "\n",
    "TEST_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/New Split Tweet Lookups/Split 10/\"\n",
    "USER = \"570927672\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.34 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Cleaned Tweet</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-23 09:05:25</td>\n",
       "      <td>[[CLS], MSDhoni, spends, some, quality, time, with, daughter, Ziva, and, we, are, just, loving, this, beautiful, click, which, captures, the, father-daughter, bond, perfectly!, [SEP]]</td>\n",
       "      <td>#MSDhoni spends some quality time with daughter #Ziva and we are just loving this beautiful click which captures the father-daughter bond perfectly!  https://t.co/shfAwdnYck</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[msdhoni, ziva]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-23 09:03:17</td>\n",
       "      <td>[[CLS], The, cabinet, approves, an, important, merger,, to, further, generate, more, employment, opportunities, and, promote, Ease, of, Doing, Business., CabinetDecisions, [SEP]]</td>\n",
       "      <td>The cabinet approves an important merger, to further generate more employment opportunities and promote Ease of Doing Business. #CabinetDecisions  https://t.co/GvVBk1QuyP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[cabinetdecisions]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-06-23 09:03:08</td>\n",
       "      <td>[[CLS], To, ensure, food, security, for, all, amidst, the, COVID19, pandemic,, Cabinet, approves, an, additional, 204, LMT, of, food, grains, for, a, period, of, further, 5, months, will, be, provided, to, NFSA, beneficiaries., CabinetDecisions, [SEP]]</td>\n",
       "      <td>To ensure food security for all amidst the #COVID19 pandemic, Cabinet approves an additional 204 LMT of food grains for a period of further 5 months will be provided to NFSA beneficiaries. #CabinetDecisions  https://t.co/3ikj3PXLiR</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[covid19, cabinetdecisions]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-06-23 09:02:52</td>\n",
       "      <td>[[CLS], Cabinet, approves, a, new, agreement, between, India, and, Saint, Vincent, &amp;amp;, The, Grenadines, to, facilitate, the, exchange, of, information, between, the, two, countries, and, to, provide, assistance, to, each, other, in, the, collection, of, tax, claims., CabinetDecisions, [SEP]]</td>\n",
       "      <td>Cabinet approves a new agreement between India and Saint Vincent &amp;amp; The Grenadines to facilitate the exchange of information between the two countries and to provide assistance to each other in the collection of tax claims. #CabinetDecisions  https://t.co/EwDk9qpEG7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[cabinetdecisions]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-06-23 09:02:42</td>\n",
       "      <td>[[CLS], Reliance, Industries, Limited, will, host, its, 44th, Annual, General, Meeting, tomorrow., Here, are, some, key, announcements, JP, Morgan, expects, to, be, made, RIL, RILAGM, RILAGM2021, Reliance, [SEP]]</td>\n",
       "      <td>Reliance Industries Limited will host its 44th Annual General Meeting tomorrow. Here are some key announcements JP Morgan expects to be made   #RIL #RILAGM  #RILAGM2021 #Reliance    https://t.co/lY727BLYWQ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[ril, rilagm, rilagm2021, reliance]</td>\n",
       "      <td>['https://www.cnbctv18.com/business/companies/reliance-agm-here-are-some-announcements-jp-morgan-expects-to-be-made-on-june-24-9756341.htm']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181666</th>\n",
       "      <td>2019-01-01 03:53:15</td>\n",
       "      <td>[[CLS], India, VIX, fell, 4.22, per, cent, to, 15.32, level., VIX, has, to, hold, below, 16, to, get, the, scope, to, surpass, its, immediate, barrier, at, 10,985., [SEP]]</td>\n",
       "      <td>India VIX fell 4.22 per cent to 15.32 level. VIX has to hold below 16 to get the scope to surpass its immediate barrier at 10,985.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181667</th>\n",
       "      <td>2019-01-01 03:53:01</td>\n",
       "      <td>[[CLS], There, was, fresh, Put, writing, was, at, strike, price, 10,800, while, Call, writing, was, seen, at, 11,100., The, option, band, signified, a, broader, trading, range, between, 10,777, and, 11,100, levels., [SEP]]</td>\n",
       "      <td>There was fresh Put writing was at strike price 10,800 while Call writing was seen at 11,100. The option band signified a broader trading range between 10,777 and 11,100 levels.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181668</th>\n",
       "      <td>2019-01-01 03:52:57</td>\n",
       "      <td>[[CLS], On, the, options, front,, maximum, Put, open, interest, was, at, 10,500, followed, by, 10,000, while, maximum, Call, OI, was, at, 11,200, followed, by, 11,000., [SEP]]</td>\n",
       "      <td>On the options front, maximum Put open interest was at 10,500 followed by 10,000 while maximum Call OI was at 11,200 followed by 11,000.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181669</th>\n",
       "      <td>2019-01-01 03:52:32</td>\n",
       "      <td>[[CLS], HIGHLIGHTS, On, the, options, front,, maximum, Put, open, interest, was, at, 10,500., Bank, Nifty, managed, to, hold, 27,000, and, witnessed, strong, momentum, Nifty, futures, closed, positive, at, 10,962, with, a, 0.42, per, cent, gain., [SEP]]</td>\n",
       "      <td>HIGHLIGHTS  On the options front, maximum Put open interest was at 10,500.  Bank Nifty managed to hold 27,000 and witnessed strong momentum  Nifty futures closed positive at 10,962 with a 0.42 per cent gain.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181679</th>\n",
       "      <td>2019-01-01 02:14:06</td>\n",
       "      <td>[[CLS], Nifty, reclaim, 10,900, led, by, HDFC, twins,, ICICI, Bank., Gainers:, Airtel, (+3%),, Infratel,, SBI, (+1.5%)., Losers:, M&amp;amp;M, (-4%),, Hindalco, (-1.5%),, ONGC, (-1%), [SEP]]</td>\n",
       "      <td>#Nifty reclaim 10,900 led by HDFC twins, ICICI Bank. Gainers: Airtel (+3%), Infratel, SBI (+1.5%). Losers: M&amp;amp;M (-4%), Hindalco (-1.5%), ONGC (-1%)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[nifty]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136545 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date                                                                                                                                                                                                                                                                                            Cleaned Tweet                                                                                                                                                                                                                                                                          tweet  language                             hashtags                                                                                                                                          urls\n",
       "0      2021-06-23 09:05:25                                                                                                                  [[CLS], MSDhoni, spends, some, quality, time, with, daughter, Ziva, and, we, are, just, loving, this, beautiful, click, which, captures, the, father-daughter, bond, perfectly!, [SEP]]                                                                                                  #MSDhoni spends some quality time with daughter #Ziva and we are just loving this beautiful click which captures the father-daughter bond perfectly!  https://t.co/shfAwdnYck       1.0                      [msdhoni, ziva]                                                                                                                                            []\n",
       "1      2021-06-23 09:03:17                                                                                                                       [[CLS], The, cabinet, approves, an, important, merger,, to, further, generate, more, employment, opportunities, and, promote, Ease, of, Doing, Business., CabinetDecisions, [SEP]]                                                                                                     The cabinet approves an important merger, to further generate more employment opportunities and promote Ease of Doing Business. #CabinetDecisions  https://t.co/GvVBk1QuyP       1.0                   [cabinetdecisions]                                                                                                                                            []\n",
       "2      2021-06-23 09:03:08                                             [[CLS], To, ensure, food, security, for, all, amidst, the, COVID19, pandemic,, Cabinet, approves, an, additional, 204, LMT, of, food, grains, for, a, period, of, further, 5, months, will, be, provided, to, NFSA, beneficiaries., CabinetDecisions, [SEP]]                                        To ensure food security for all amidst the #COVID19 pandemic, Cabinet approves an additional 204 LMT of food grains for a period of further 5 months will be provided to NFSA beneficiaries. #CabinetDecisions  https://t.co/3ikj3PXLiR       1.0          [covid19, cabinetdecisions]                                                                                                                                            []\n",
       "3      2021-06-23 09:02:52  [[CLS], Cabinet, approves, a, new, agreement, between, India, and, Saint, Vincent, &amp;, The, Grenadines, to, facilitate, the, exchange, of, information, between, the, two, countries, and, to, provide, assistance, to, each, other, in, the, collection, of, tax, claims., CabinetDecisions, [SEP]]  Cabinet approves a new agreement between India and Saint Vincent &amp; The Grenadines to facilitate the exchange of information between the two countries and to provide assistance to each other in the collection of tax claims. #CabinetDecisions  https://t.co/EwDk9qpEG7       1.0                   [cabinetdecisions]                                                                                                                                            []\n",
       "4      2021-06-23 09:02:42                                                                                     [[CLS], Reliance, Industries, Limited, will, host, its, 44th, Annual, General, Meeting, tomorrow., Here, are, some, key, announcements, JP, Morgan, expects, to, be, made, RIL, RILAGM, RILAGM2021, Reliance, [SEP]]                                                                  Reliance Industries Limited will host its 44th Annual General Meeting tomorrow. Here are some key announcements JP Morgan expects to be made   #RIL #RILAGM  #RILAGM2021 #Reliance    https://t.co/lY727BLYWQ       1.0  [ril, rilagm, rilagm2021, reliance]  ['https://www.cnbctv18.com/business/companies/reliance-agm-here-are-some-announcements-jp-morgan-expects-to-be-made-on-june-24-9756341.htm']\n",
       "...                    ...                                                                                                                                                                                                                                                                                                      ...                                                                                                                                                                                                                                                                            ...       ...                                  ...                                                                                                                                           ...\n",
       "181666 2019-01-01 03:53:15                                                                                                                              [[CLS], India, VIX, fell, 4.22, per, cent, to, 15.32, level., VIX, has, to, hold, below, 16, to, get, the, scope, to, surpass, its, immediate, barrier, at, 10,985., [SEP]]                                                                                                                                             India VIX fell 4.22 per cent to 15.32 level. VIX has to hold below 16 to get the scope to surpass its immediate barrier at 10,985.       1.0                                   []                                                                                                                                            []\n",
       "181667 2019-01-01 03:53:01                                                                           [[CLS], There, was, fresh, Put, writing, was, at, strike, price, 10,800, while, Call, writing, was, seen, at, 11,100., The, option, band, signified, a, broader, trading, range, between, 10,777, and, 11,100, levels., [SEP]]                                                                                              There was fresh Put writing was at strike price 10,800 while Call writing was seen at 11,100. The option band signified a broader trading range between 10,777 and 11,100 levels.       1.0                                   []                                                                                                                                            []\n",
       "181668 2019-01-01 03:52:57                                                                                                                          [[CLS], On, the, options, front,, maximum, Put, open, interest, was, at, 10,500, followed, by, 10,000, while, maximum, Call, OI, was, at, 11,200, followed, by, 11,000., [SEP]]                                                                                                                                       On the options front, maximum Put open interest was at 10,500 followed by 10,000 while maximum Call OI was at 11,200 followed by 11,000.       1.0                                   []                                                                                                                                            []\n",
       "181669 2019-01-01 03:52:32                                            [[CLS], HIGHLIGHTS, On, the, options, front,, maximum, Put, open, interest, was, at, 10,500., Bank, Nifty, managed, to, hold, 27,000, and, witnessed, strong, momentum, Nifty, futures, closed, positive, at, 10,962, with, a, 0.42, per, cent, gain., [SEP]]                                                                HIGHLIGHTS  On the options front, maximum Put open interest was at 10,500.  Bank Nifty managed to hold 27,000 and witnessed strong momentum  Nifty futures closed positive at 10,962 with a 0.42 per cent gain.       1.0                                   []                                                                                                                                            []\n",
       "181679 2019-01-01 02:14:06                                                                                                               [[CLS], Nifty, reclaim, 10,900, led, by, HDFC, twins,, ICICI, Bank., Gainers:, Airtel, (+3%),, Infratel,, SBI, (+1.5%)., Losers:, M&amp;M, (-4%),, Hindalco, (-1.5%),, ONGC, (-1%), [SEP]]                                                                                                                         #Nifty reclaim 10,900 led by HDFC twins, ICICI Bank. Gainers: Airtel (+3%), Infratel, SBI (+1.5%). Losers: M&amp;M (-4%), Hindalco (-1.5%), ONGC (-1%)       1.0                              [nifty]                                                                                                                                            []\n",
       "\n",
       "[136545 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "add_CT_dummies(READ_PATH_ROOT=TEST_PATH_ROOT, DF_SAVE_PATH_ROOT=\"0\", BERT_INPUT_SAVE_PATH_ROOT=\"0\", USER=USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.5 s ± 153 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit OLD_add_CT_dummies(READ_PATH_ROOT=TEST_PATH_ROOT, DF_SAVE_PATH_ROOT=\"0\", BERT_INPUT_SAVE_PATH_ROOT=\"0\", USER=USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def OLD_add_CT_dummies(READ_PATH_ROOT, DF_SAVE_PATH_ROOT, BERT_INPUT_SAVE_PATH_ROOT, USER):\n",
    "    \n",
    "#     READ_PATH = READ_PATH_ROOT + USER + '_TWEETS.csv'\n",
    "#     DF_SAVE_PATH = DF_SAVE_PATH_ROOT + USER + '_CLEANED_TWEETS.csv'\n",
    "#     BERT_INPUT_SAVE_PATH = BERT_INPUT_SAVE_PATH_ROOT + USER + '_BERT_INPUT.csv'\n",
    "    \n",
    "#     df = pd.read_csv(READ_PATH, parse_dates=['date'], converters={'hashtags': eval})#.set_index('retweet')\n",
    "    \n",
    "#     if df['language'].sum() / len(df) < .1: #if less than 10% of tweets are recognized as English\n",
    "    \n",
    "#         list_of_non_english_users.append(USER)\n",
    "    \n",
    "#     else:\n",
    "        \n",
    "#         df.insert(loc=1, column='Cleaned Tweet', value=df['tweet'].apply(clean_tweet))\n",
    "        \n",
    "#         df = df[ df['Cleaned Tweet'].map(len) > 4 ] #only keep tweets with more than 4 words\n",
    "        \n",
    "#         del df['Unnamed: 0']\n",
    "\n",
    "#     #     df['hashtags'] = df['hashtags'].apply(literal_eval)\n",
    "\n",
    "#         # create dummies for each hashtag and link (in each tweet)    \n",
    "#         for link in CT_link_list:\n",
    "#             df[f'Link - {link}'] = df['urls'].str.contains(link, case=False).astype(int)\n",
    "\n",
    "#         for tag in general_conspiracy_hashtags:\n",
    "#             df[f'Tag - {tag}'] = df['hashtags'].apply(hashtag_in_list).astype(int)\n",
    "\n",
    "#         for keyword in keywords:\n",
    "#             df[f'Keyword - {keyword}'] = df['tweet'].str.contains(keyword, case=False).astype(int)\n",
    "\n",
    "#         df.insert(loc=0, column='CT Tweet (Dummy)', value=(df[link_tag_keyword_cols].sum(axis=1) > 0).astype(int))\n",
    "\n",
    "        \n",
    "\n",
    "#         # somewhat unrelated - fix broken 'retweet' boolean column\n",
    "#         df['retweet'] = df['tweet'].str.startswith('RT').astype(int)\n",
    "\n",
    "#         return df\n",
    "\n",
    "# #         df.to_csv(DF_SAVE_PATH, index=False)\n",
    "\n",
    "# #         df[df['language']==1][['Cleaned Tweet', 'CT Tweet (Dummy)']].rename(columns={'Cleaned Tweet':'sentence1', 'CT Tweet (Dummy)':'label'}).to_csv(BERT_INPUT_SAVE_PATH, index=False)\n",
    "\n",
    "# #         return df[['Cleaned Tweet', 'CT Tweet (Dummy)']].rename(columns={'Cleaned Tweet':'sentence1', 'CT Tweet (Dummy)':'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Root to Create .CSVs for BERT Input\n",
    "\n",
    "sentence1, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_CSV_folder_path = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/BERT/BERT Input CSVs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Final Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000027408508977152\n",
      "1000104162292617221\n",
      "1000269175401541632\n",
      "1000372897\n",
      "10003862\n",
      "1000459433762279425\n",
      "1000563162016432128\n",
      "1000736389246472193\n",
      "1000743191308484608\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_of_non_english_USERs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-c4f3892017f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[1;32min\u001b[0m \u001b[0musers_in_this_split\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0madd_CT_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAD_PATH_ROOT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_read_path_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDF_SAVE_PATH_ROOT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_write_path_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBERT_INPUT_SAVE_PATH_ROOT\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBERT_CSV_folder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUSER\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-143-b03bb909400c>\u001b[0m in \u001b[0;36madd_CT_dummies\u001b[1;34m(READ_PATH_ROOT, DF_SAVE_PATH_ROOT, BERT_INPUT_SAVE_PATH_ROOT, USER)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m.1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#if less than 10% of tweets are recognized as English\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mlist_of_non_english_USERs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUSER\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'list_of_non_english_USERs' is not defined"
     ]
    }
   ],
   "source": [
    "# folder containing folders of all split user lookups\n",
    "OG_READ_PATH_ROOT = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/All Conspiracy Tweeters' Tweets/\"\n",
    "\n",
    "# folder which will contain folders of all split user lookup AGGREGATIONS (weekly CT activity)\n",
    "OG_WRITE_PATH_ROOT = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Looking Up All Conspiracy Hashtag User Tweets/Labeled OG Tweets/\"\n",
    "\n",
    "og_splits = [5,6,7,8,9]\n",
    "\n",
    "for split in og_splits:\n",
    "    local_read_path_root = OG_READ_PATH_ROOT + f'Split {split}/'\n",
    "    local_write_path_root = OG_WRITE_PATH_ROOT + f'Split {split}/'\n",
    "    \n",
    "    # make new folders to store processed tweets\n",
    "    if not os.path.exists(local_write_path_root):\n",
    "        os.makedirs(local_write_path_root)\n",
    "    \n",
    "    # find all users who have been searched and stored in this folder\n",
    "    users_in_this_split = [int(f.split('_')[0]) for f in listdir(local_read_path_root) if isfile(join(local_read_path_root, f))]\n",
    "\n",
    "    # aggregate each user's tweets\n",
    "    for user in users_in_this_split:\n",
    "        print(user)\n",
    "        add_CT_dummies(READ_PATH_ROOT=local_read_path_root, DF_SAVE_PATH_ROOT=local_write_path_root, BERT_INPUT_SAVE_PATH_ROOT=BERT_CSV_folder_path, USER=str(user))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
