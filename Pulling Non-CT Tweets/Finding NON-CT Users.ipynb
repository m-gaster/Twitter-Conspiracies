{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import twint\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta, date\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from collections import Counter\n",
    "from ast import literal_eval\n",
    "\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Non-CT Training Data/Non-CT Tweets by Date Range/\"    #folder where non-CT tweets from date ranges will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "    local_df = df\n",
    "    \n",
    "    #drop redundant cols\n",
    "    local_df.drop(['created_at', 'timezone', 'cashtags', 'user_id_str', 'photos', 'video', 'thumbnail', 'translate', 'trans_src', 'trans_dest', 'name', 'search'], axis=1, inplace=True)\n",
    "    \n",
    "    #convert date to datetime\n",
    "    local_df['date'] = pd.to_datetime(local_df['date'])\n",
    "    \n",
    "    return local_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tweets_in_date_range(search_terms, start_date, end_date, LOCAL_SAVE_PATH_ROOT):\n",
    "    '''    \n",
    "    search_terms = list of strings (or singleton) of search terms that will be in the tweet\n",
    "    \n",
    "    returns a pandas dataframe of tweets\n",
    "    '''\n",
    "    \n",
    "    search_string = \" OR \".join(search_terms)\n",
    "    \n",
    "    c = twint.Config()\n",
    "    \n",
    "#     date range of search\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    \n",
    "    c.Search = search_string\n",
    "    \n",
    "    # only collect a certain number of tweets\n",
    "#     c.Limit = num_tweets\n",
    "\n",
    "    # Don't print output\n",
    "    c.Hide_output = True\n",
    "    \n",
    "    # find shadow-banned accounts too - this apparently slows things down considerably\n",
    "    c.Profile_full = True\n",
    "\n",
    "#     c.Output = f\"{num_tweets} tweets - {start_date} to {end_date}.csv\"\n",
    "    c.Pandas = True\n",
    "\n",
    "    twint.run.Search(c)\n",
    "\n",
    "    clean_tweets(twint.storage.panda.Tweets_df).to_csv(fr\"{LOCAL_SAVE_PATH_ROOT}{start_date} to {end_date}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2021-01-10', '2021-01-16')\n",
      "('2021-01-17', '2021-01-23')\n",
      "('2021-01-24', '2021-01-30')\n",
      "('2021-01-31', '2021-02-06')\n",
      "('2021-02-07', '2021-02-13')\n",
      "('2021-02-14', '2021-02-20')\n",
      "('2021-02-21', '2021-02-27')\n",
      "('2021-02-28', '2021-03-06')\n",
      "('2021-03-07', '2021-03-13')\n",
      "('2021-03-14', '2021-03-20')\n",
      "('2021-03-21', '2021-03-27')\n",
      "('2021-03-28', '2021-04-03')\n",
      "('2021-04-04', '2021-04-10')\n",
      "('2021-04-11', '2021-04-17')\n",
      "('2021-04-18', '2021-04-24')\n",
      "('2021-04-25', '2021-05-01')\n",
      "('2021-05-02', '2021-05-08')\n",
      "('2021-05-09', '2021-05-15')\n",
      "('2021-05-16', '2021-05-22')\n",
      "('2021-05-23', '2021-05-29')\n",
      "('2021-05-30', '2021-06-05')\n",
      "('2021-06-06', '2021-06-12')\n",
      "('2021-06-13', '2021-06-19')\n",
      "('2021-06-20', '2021-06-26')\n",
      "('2021-06-27', '2021-07-03')\n",
      "('2021-07-04', '2021-07-10')\n"
     ]
    }
   ],
   "source": [
    "date_ranges = []\n",
    "\n",
    "sixdays = timedelta(days=6)\n",
    "\n",
    "today_str = date.today().strftime('%m-%d-%Y')\n",
    "\n",
    "for date in pd.date_range(start='2021-01-09', end=today_str, freq='1W'):\n",
    "    date_tuple = ((date.strftime('%Y-%m-%d'), (date+sixdays).strftime('%Y-%m-%d') ))\n",
    "    \n",
    "    print(date_tuple)\n",
    "    \n",
    "    date_ranges.append(date_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find associated hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIST OF NON-CT KEYWORDS/HASHTAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_CT_words_and_tags = [\n",
    "    'essentialworkers',\n",
    "    'pfizerproud',\n",
    "    'vaccineswork',\n",
    "    'sciencewillwin'\n",
    "]\n",
    "\n",
    "non_CT_words_and_tags = non_CT_words_and_tags + [f'#{x}' for x in non_CT_words_and_tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Searching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2021-01-10', '2021-01-16')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-01-17', '2021-01-23')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-01-24', '2021-01-30')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-01-31', '2021-02-06')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-02-07', '2021-02-13')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-02-14', '2021-02-20')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-02-21', '2021-02-27')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-02-28', '2021-03-06')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-03-07', '2021-03-13')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-03-14', '2021-03-20')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-03-21', '2021-03-27')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-03-28', '2021-04-03')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-04-04', '2021-04-10')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-04-11', '2021-04-17')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-04-18', '2021-04-24')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-04-25', '2021-05-01')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-05-02', '2021-05-08')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-05-09', '2021-05-15')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-05-16', '2021-05-22')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-05-23', '2021-05-29')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-05-30', '2021-06-05')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-06-06', '2021-06-12')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-06-13', '2021-06-19')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-06-20', '2021-06-26')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "('2021-06-27', '2021-07-03')\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    }
   ],
   "source": [
    "for date_tup in date_ranges:\n",
    "    print(date_tup)\n",
    "    find_tweets_in_date_range(search_terms=non_CT_words_and_tags, start_date=date_tup[0], end_date=date_tup[1], LOCAL_SAVE_PATH_ROOT=SAVE_PATH_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_CT_tweets_by_date_filenames = [f for f in listdir(SAVE_PATH_ROOT) if isfile(join(SAVE_PATH_ROOT, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags_list(series):\n",
    "    \n",
    "    local_series = series.apply(literal_eval)\n",
    "    \n",
    "    local_series = local_series.explode()\n",
    "    \n",
    "    return local_series[pd.notna(local_series)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = Counter({})\n",
    "hashtags = Counter({})\n",
    "num_tweets = 0\n",
    "\n",
    "for filename in non_CT_tweets_by_date_filenames:\n",
    "    try:\n",
    "        full_path = fr'{SAVE_PATH_ROOT}{filename}'\n",
    "        df = pd.read_csv(full_path)\n",
    "        \n",
    "        id_count = Counter(df['user_id'].value_counts().to_dict())\n",
    "        user_ids = user_ids + id_count\n",
    "        \n",
    "        \n",
    "        hashtag_list = get_hashtags_list(df['hashtags'])\n",
    "        hashtag_count = Counter(hashtag_list.value_counts().to_dict())\n",
    "        hashtags = hashtag_count + hashtags\n",
    "        \n",
    "        num_tweets += len(df)\n",
    "        \n",
    "        del df    \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(Exception)\n",
    "        print(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_NON_CT_USERS = pd.DataFrame.from_dict(user_ids, orient='index', columns=['Num NON-CT Tweets'])\n",
    "\n",
    "ALL_NON_CT_USERS = ALL_NON_CT_USERS.reset_index()\n",
    "\n",
    "ALL_NON_CT_USERS = ALL_NON_CT_USERS.rename(columns={'index': 'ID'})\n",
    "\n",
    "ALL_NON_CT_USERS['Searched'] = 0\n",
    "\n",
    "ALL_NON_CT_USERS.sort_values('Num NON-CT Tweets', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove users who we flagged as CT users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Geolocated Master CT User List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_USER_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Geo Cross Referencing/Master User List - GEOLOCATED.csv\"\n",
    "\n",
    "ALL_CT_USERS = pd.read_csv(MASTER_USER_PATH).set_index('ID')\n",
    "\n",
    "intersection_of_CT_and_NON_CT_users = np.intersect1d(ALL_CT_USERS.index, ALL_NON_CT_USERS['ID'])\n",
    "\n",
    "ALL_CT_USERS['In CT & Non-CT Groups'] = 0\n",
    "\n",
    "ALL_CT_USERS['In CT & Non-CT Groups'].loc[intersection_of_CT_and_NON_CT_users] = 1\n",
    "\n",
    "ALL_CT_USERS.to_csv(MASTER_USER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Non-CT User List (the one we just made in this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_NON_CT_USERS.set_index('ID', inplace=True)\n",
    "\n",
    "ALL_NON_CT_USERS['In CT & Non-CT Groups'] = 0\n",
    "\n",
    "ALL_NON_CT_USERS['In CT & Non-CT Groups'].loc[intersection_of_CT_and_NON_CT_users] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_NON_CT_USERS.to_csv(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Non-CT Training Data/Non-CT Users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEXT: Look Up All Tweets from Non-CT Users (if not in both datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Non-CT Training Data/Non-CT Tweets by Date Range/2021-04-25 to 2021-05-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tweets_in_date_range(search_terms, start_date, end_date, LOCAL_SAVE_PATH_ROOT):\n",
    "    '''    \n",
    "    search_terms = list of strings (or singleton) of search terms that will be in the tweet\n",
    "    \n",
    "    returns a pandas dataframe of tweets\n",
    "    '''\n",
    "    \n",
    "    search_string = \" OR \".join(search_terms)\n",
    "    \n",
    "    c = twint.Config()\n",
    "    \n",
    "#     date range of search\n",
    "    c.Since = start_date\n",
    "    c.Until = end_date\n",
    "    \n",
    "    c.Search = search_string\n",
    "    \n",
    "    # only collect a certain number of tweets\n",
    "#     c.Limit = num_tweets\n",
    "\n",
    "    # Don't print output\n",
    "    c.Hide_output = True\n",
    "    \n",
    "    # find shadow-banned accounts too - this apparently slows things down considerably\n",
    "    c.Profile_full = True\n",
    "\n",
    "#     c.Output = f\"{num_tweets} tweets - {start_date} to {end_date}.csv\"\n",
    "    c.Pandas = True\n",
    "\n",
    "    twint.run.Search(c)\n",
    "\n",
    "    return clean_tweets(twint.storage.panda.Tweets_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#believescience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_search_terms = ['#flattenthecurve', '#maskssavelives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    }
   ],
   "source": [
    "test = find_tweets_in_date_range(search_terms=test_search_terms, start_date='2020-01-01', end_date='2021-07-07', LOCAL_SAVE_PATH_ROOT=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(r\"#flattenthecurve, #maskssavelives.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Non-CT Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Users Who Are Also Flagged for CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_overlapped_users(df1, df2):\n",
    "    '''\n",
    "    Both DataFrames must have user ID (or something like that) as index\n",
    "    '''\n",
    "    \n",
    "    intersection_of_users = np.intersect1d(df1.index, df2.index)\n",
    "    \n",
    "    df1['In CT and Non-CT DFs'] = 0\n",
    "    df2['In CT and Non-CT DFs'] = 0\n",
    "    \n",
    "    df1['In CT and Non-CT DFs'].loc[intersection_of_users] = 1\n",
    "    df2['In CT and Non-CT DFs'].loc[intersection_of_users] = 1\n",
    "\n",
    "    return df1, df2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_USER_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Geo Cross Referencing/Master User List - GEOLOCATED.csv\"\n",
    "\n",
    "ALL_CT_USERS = pd.read_csv(MASTER_USER_PATH).set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "flattenthecurve = pd.read_csv(r\"#flattenthecurve, #maskssavelives.csv\")\n",
    "\n",
    "flattenthecurve = flattenthecurve.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "flattenthecurve, ALL_CT_USERS = label_overlapped_users(flattenthecurve, ALL_CT_USERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>date</th>\n",
       "      <th>place</th>\n",
       "      <th>tweet</th>\n",
       "      <th>language</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>link</th>\n",
       "      <th>urls</th>\n",
       "      <th>retweet</th>\n",
       "      <th>nlikes</th>\n",
       "      <th>nreplies</th>\n",
       "      <th>nretweets</th>\n",
       "      <th>quote_url</th>\n",
       "      <th>near</th>\n",
       "      <th>geo</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>In CT and Non-CT DFs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1412197378707267585</th>\n",
       "      <td>1412197378707267585</td>\n",
       "      <td>2021-07-05 16:51:07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#COVID19_Kitakyushucity  新規感染者数　6人 #StayHome #StaySafe  #FlattenTheCurve  #BeatCovid19  #StopCovid19</td>\n",
       "      <td>ja</td>\n",
       "      <td>['covid19_kitakyushucity', 'stayhome', 'staysafe', 'flattenthecurve', 'beatcovid19', 'stopcovid19']</td>\n",
       "      <td>267059579</td>\n",
       "      <td>boyackey_badmen</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>https://twitter.com/boyackey_badmen/status/1412197378707267585</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/tokki_kitaq/status/1412017867910828042</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[]</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412182760186421251</th>\n",
       "      <td>1412182760186421251</td>\n",
       "      <td>2021-07-05 15:53:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#UprootTheSystem #nomoreemptypromises  #climatestrike #FridaysForFuture #schoolstrike4climate #workstrike4climate  #flattenthecurve #FaceTheClimateEmergency #FightClimateInjustice  https://t.co/z...</td>\n",
       "      <td>und</td>\n",
       "      <td>['uprootthesystem', 'nomoreemptypromises', 'climatestrike', 'fridaysforfuture', 'schoolstrike4climate', 'workstrike4climate', 'flattenthecurve', 'facetheclimateemergency', 'fightclimateinjustice']</td>\n",
       "      <td>88551890</td>\n",
       "      <td>EricBourgouin</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>https://twitter.com/EricBourgouin/status/1412182760186421251</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>[]</td>\n",
       "      <td>nan</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         conversation_id                 date place  \\\n",
       "id                                                                    \n",
       "1412197378707267585  1412197378707267585  2021-07-05 16:51:07   NaN   \n",
       "1412182760186421251  1412182760186421251  2021-07-05 15:53:01   NaN   \n",
       "\n",
       "                                                                                                                                                                                                                      tweet  \\\n",
       "id                                                                                                                                                                                                                            \n",
       "1412197378707267585                                                                                                    #COVID19_Kitakyushucity  新規感染者数　6人 #StayHome #StaySafe  #FlattenTheCurve  #BeatCovid19  #StopCovid19   \n",
       "1412182760186421251  #UprootTheSystem #nomoreemptypromises  #climatestrike #FridaysForFuture #schoolstrike4climate #workstrike4climate  #flattenthecurve #FaceTheClimateEmergency #FightClimateInjustice  https://t.co/z...   \n",
       "\n",
       "                    language  \\\n",
       "id                             \n",
       "1412197378707267585       ja   \n",
       "1412182760186421251      und   \n",
       "\n",
       "                                                                                                                                                                                                                 hashtags  \\\n",
       "id                                                                                                                                                                                                                          \n",
       "1412197378707267585                                                                                                   ['covid19_kitakyushucity', 'stayhome', 'staysafe', 'flattenthecurve', 'beatcovid19', 'stopcovid19']   \n",
       "1412182760186421251  ['uprootthesystem', 'nomoreemptypromises', 'climatestrike', 'fridaysforfuture', 'schoolstrike4climate', 'workstrike4climate', 'flattenthecurve', 'facetheclimateemergency', 'fightclimateinjustice']   \n",
       "\n",
       "                       user_id         username  day  hour  \\\n",
       "id                                                           \n",
       "1412197378707267585  267059579  boyackey_badmen    1    16   \n",
       "1412182760186421251   88551890    EricBourgouin    1    15   \n",
       "\n",
       "                                                                               link  \\\n",
       "id                                                                                    \n",
       "1412197378707267585  https://twitter.com/boyackey_badmen/status/1412197378707267585   \n",
       "1412182760186421251    https://twitter.com/EricBourgouin/status/1412182760186421251   \n",
       "\n",
       "                    urls  retweet  nlikes  nreplies  nretweets  \\\n",
       "id                                                               \n",
       "1412197378707267585   []    False       0         0          0   \n",
       "1412182760186421251   []    False       5         0          3   \n",
       "\n",
       "                                                                      quote_url  \\\n",
       "id                                                                                \n",
       "1412197378707267585  https://twitter.com/tokki_kitaq/status/1412017867910828042   \n",
       "1412182760186421251                                                         NaN   \n",
       "\n",
       "                     near  geo  source  user_rt_id  user_rt  retweet_id  \\\n",
       "id                                                                        \n",
       "1412197378707267585   nan  nan     nan         nan      nan         nan   \n",
       "1412182760186421251   nan  nan     nan         nan      nan         nan   \n",
       "\n",
       "                    reply_to  retweet_date  In CT and Non-CT DFs  \n",
       "id                                                                \n",
       "1412197378707267585       []           nan                     0  \n",
       "1412182760186421251       []           nan                     0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattenthecurve.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Label Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword lists and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_conspiracy_hashtags = [\n",
    "    'plandemic',\n",
    "    'scamdemic',\n",
    "    'covidhoax',\n",
    "    'nwo',\n",
    "    'covid1984',\n",
    "    'plandemia',\n",
    "    'agenda21',\n",
    "    'thegreatreset',\n",
    "    'agenda2030',\n",
    "    'newworldorder',\n",
    "    'wakeupamerica',\n",
    "#     'wakeup',\n",
    "    'openamericanow',\n",
    "    'firefauci',\n",
    "    'wwg1wga',\n",
    "    'qanon',\n",
    "    'coronahoax'\n",
    "]\n",
    "\n",
    "keywords = [\n",
    "    'plandemic',\n",
    "    'scamdemic',\n",
    "    'covidhoax',\n",
    "    'covid hoax',\n",
    "    'covid1984',\n",
    "    'plandemia',\n",
    "    'new world order',\n",
    "    'wake up america',\n",
    "    'open america now',\n",
    "    'fire fauci',\n",
    "    'wwg1wga',\n",
    "    'qanon',\n",
    "    'coronahoax',\n",
    "    'corona hoax',\n",
    "]\n",
    "\n",
    "CT_link_list = ['zerohedge.com', 'infowars.com', 'principia-scientific.com',\n",
    "'tx.voice-truth.com', 'humansarefree.com', 'activistpost.com'\n",
    "'gnews.org', 'wakingtimes.com', 'brighteon.com','thewallwillfall.org','sott.net',]\n",
    "\n",
    "\n",
    "hashtag_set = set(['#' + tag for tag in general_conspiracy_hashtags])\n",
    "keyword_set = set(keywords)\n",
    "\n",
    "re_escape_keywords = '|'.join([re.escape(word) for word in keywords])\n",
    "re_escape_links = '|'.join([re.escape(link) for link in CT_link_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions for Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_in_list(list_of_hashtags_in_tweet):\n",
    "    return any(hashtag.upper() in [tag.upper() for tag in list_of_hashtags_in_tweet] for hashtag in general_conspiracy_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    input = tweet (str)\n",
    "    output = cleaned_tweet(str)\n",
    "    '''\n",
    "    \n",
    "    return [['CLS']] + [x.replace('#','') for x in tweet.split() if not (x.startswith(('http','@')) or x in keyword_set or x in hashtag_set)] + [['SEP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_CT_tweets(df):\n",
    "    \n",
    "    return ( df['tweet'].str.contains(re_escape_keywords, case=False) | df['urls'].str.contains(re_escape_links, case=False) | df['hashtags'].apply(hashtag_in_list)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenthecurve['CT Tweet'] = label_CT_tweets(flattenthecurve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenthecurve = flattenthecurve[flattenthecurve['language']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenthecurve.to_csv(r\"Semi-Cleaned #flattenthecurve, #maskssavelives.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "flattenthecurve = pd.read_csv(r\"Semi-Cleaned #flattenthecurve, #maskssavelives.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenthecurve['Cleaned Tweet'] = flattenthecurve['tweet'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenthecurve[(flattenthecurve['Cleaned Tweet'].map(len) > 4)][['Cleaned Tweet', 'CT Tweet']].to_csv(r'#flattenthecurve, #maskssavelives - CLEANED FOR BERT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9924777969562723"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(flattenthecurve['Cleaned Tweet'].map(len) > 4).sum() / len(flattenthecurve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
