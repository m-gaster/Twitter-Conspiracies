{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9adec31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5e6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing list of users and their locations\n",
    "USER_LOC_PATH = r\"C:/Users/lukea/Documents/GitHub/Twitter-Conspiracies/Pulling Twitter Data/All User IDs.csv\"\n",
    "\n",
    "# Top level folder containing tweets by user\n",
    "TWEETS_PATH = r\"G:/Twitter-Conspiracies/user-tweets\"\n",
    "\n",
    "# CHANGE THIS to reflect the splits you are using. Note that range(0,5) gives 0,1,2,3,4\n",
    "splits = range(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40fa1701",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_conspiracy_hashtags = [\n",
    "    'plandemic',\n",
    "    'scamdemic',\n",
    "    'covidhoax',\n",
    "    'nwo',\n",
    "    'covid1984',\n",
    "    'plandemia',\n",
    "    'agenda21',\n",
    "    'thegreatreset',\n",
    "    'agenda2030',\n",
    "    'newworldorder',\n",
    "    'wakeupamerica',\n",
    "    'wakeup',\n",
    "    'openamericanow',\n",
    "    'firefauci',\n",
    "    'wwg1wga',\n",
    "    'qanon',\n",
    "    'coronahoax'\n",
    "]\n",
    "nan_value = float(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "7512be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hashtags(df):\n",
    "    '''\n",
    "    This removes extra characters from the hashtag column and splits multiple hashtags into list items\n",
    "    ---\n",
    "    df: csv file pulled from Twint\n",
    "    '''\n",
    "    df_new = df.copy(deep=True)\n",
    "    df_clean = pd.DataFrame(row.replace(\"'\",'').replace(\"[\",'').replace(\"]\",'').replace(\"]\",'').replace(\" \",'') for row in df_new['hashtags'])\n",
    "    df_new['hashtags'] = df_clean\n",
    "    df_split = split_mults(df_new)\n",
    "    return df_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ef6a5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mults(df):\n",
    "    '''\n",
    "    Splits multiple hashtags into list items\n",
    "    ---\n",
    "    df: csv file pulled from Twint\n",
    "    '''\n",
    "    \n",
    "    df_new = df.copy(deep=True)\n",
    "    df_split = [row.split(\",\") for row in df_new['hashtags']]\n",
    "    df_new['hashtags'] = df_split\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "0747ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_non_CT_hashtags(df,hashlist):\n",
    "    '''\n",
    "    Generates a new dataframe with only the rows that include tweets from our list of CT hashtags\n",
    "    ---\n",
    "    df: csv file pulled from Twint including all of a user's tweets\n",
    "    hashlist: list of CT hashtags\n",
    "    '''\n",
    "    \n",
    "    df = process_hashtags(df)\n",
    "    df_new = df.copy(deep=True).drop(df.index) #empty df to be populated (done this way so that indices are correct)\n",
    "    \n",
    "    # ~NOTE~ I think this part is quite slow. It would be faster to do this with list comprehension.\n",
    "    for row in range(0,len(df['hashtags'])):\n",
    "        if any(item in df['hashtags'][row] for item in hashlist):\n",
    "            df_new = df_new.append(pd.DataFrame(df.loc[row]).transpose())\n",
    "            \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "06b84e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_count(df):\n",
    "    '''\n",
    "    Generates a dataframe with the number of tweets on a given day from a user. \n",
    "    Can be used with either the full dataframe of tweets, or the dataframe of CT tweets generated by drop_non_CT_hashtags()\n",
    "    ---\n",
    "    df: csv file pulled from Twint including some or all of a user's tweets\n",
    "    '''\n",
    "    \n",
    "    df['day'] = pd.DataFrame(row.split(' ')[0] for row in df['date']) # Drop time information\n",
    "    dates = pd.DataFrame(df['day']).drop_duplicates() # get unique dates\n",
    "    dates['tweet_count'] = 0\n",
    "    \n",
    "    for day in dates['day']:\n",
    "        dates.loc[dates['day']==day, 'tweet_count'] = len(df.loc[df['day'] == day]) # set tweet count for each unique date according to total number of tweets from that day\n",
    "    \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2babab3e",
   "metadata": {},
   "source": [
    "The following chunk measures processing time for a single user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "d9fd62a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.84382110000297"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USER_PATH = r'G:/Twitter-Conspiracies/user-tweets/2/14125938_TWEETS.csv'\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "user_tweets = pd.read_csv(USER_PATH)\n",
    "all_tweet_count = get_daily_count(user_tweets)\n",
    "CT_tweet_count = get_daily_count(drop_non_CT_hashtags(user_tweets,general_conspiracy_hashtags).reset_index())\n",
    "\n",
    "stop = time.perf_counter()\n",
    "\n",
    "stop - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9882c1c",
   "metadata": {},
   "source": [
    "The above result indicates that we should probably improve efficiency before actually running this. At nearly 8 seconds per user, we'll only get through about 10k per day. This is most likely a CPU bottleneck, not a network thing, so running parallel won't help as much.\n",
    "\n",
    "The likely culprits are:\n",
    "1. looping through each row to check the hashtags in drop_non_CT_tweets()\n",
    "    - Can improve this somewhat by dropping tweets with no hashtags at all before we loop, if nothing else\n",
    "2. sloppy code that copies the dataframe several times more than is probably necessary\n",
    "    - Need to go through and figure out where this can be pared down\n",
    "\n",
    "The next step is to create a dataframe to hold all of the tweet count data, and then to loop through all users and add them to it.\n",
    "\n",
    "Finally, we need to aggregate users by county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bfbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton of loop code\n",
    "'''\n",
    "for i in splits:\n",
    "    SUB_PATH = TWEETS_PATH + \"/\" + str(i)\n",
    "    \n",
    "    for filename in os.listdir(SUB_PATH):\n",
    "        user_tweets = pd.read_csv(SUB_PATH + filename)\n",
    "        all_tweet_count = get_daily_count(user_tweets)\n",
    "        CT_tweet_count = get_daily_count(drop_non_CT_hashtags(split_mults(remove_extra_chars(user_tweets)), general_conspiracy_hashtags))\n",
    "        \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
