{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension codefolding/main...\n",
      "      - Validating: problems found:\n",
      "        - require?  X codefolding/main\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable codefolding/main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Identify US users \n",
    "2. Identify US Tweets \n",
    "3. Delete all obs. without US User or Tweet \n",
    "4. Match to list of users we collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GeoCov19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/geo_feb_01_10/geo_2020-02-01/geo_2020-02-01.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_uppercase(d):\n",
    "    '''\n",
    "    input = dictionary\n",
    "    output = uppercase dictionary\n",
    "    '''\n",
    "    \n",
    "    return {key.upper(): value.upper() for key,value in d.items()}\n",
    "\n",
    "df['user_location'] = df['user_location'].apply(make_dict_uppercase)\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "def make_list_of_dicts_uppercase(list_of_dicts):\n",
    "    '''\n",
    "    input = list of dictionaries\n",
    "    output = uppercase list of dictionaries\n",
    "    '''\n",
    "    \n",
    "    return [make_dict_uppercase(d) for d in list_of_dicts]\n",
    "\n",
    "\n",
    "df['tweet_locations'] = df['tweet_locations'].apply(make_list_of_dicts_uppercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get (State & County) --> FIPS dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# import data from wikipedia\n",
    "fips = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\")[1]\n",
    "\n",
    "# remove all hyperlinks (these look like \"... County [h]\", etc.)\n",
    "fips['County or equivalent'] = fips['County or equivalent'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "\n",
    "# convert to uppercase\n",
    "fips['County or equivalent'] = fips['County or equivalent'].apply(lambda x: x.upper())\n",
    "fips['State or equivalent'] = fips['State or equivalent'].apply(lambda x: x.upper())\n",
    "\n",
    "# replace \"St.\" with \"Saint\"\n",
    "fips['County or equivalent'] = [x.replace('ST.','SAINT') for x in fips['County or equivalent']]\n",
    "\n",
    "# remove everything after a comma in a county name (e.g. \"ANCHORAGE, MUNICIPALITY OF\")\n",
    "fips['County or equivalent'] = [x.split(',')[0] for x in fips['County or equivalent']]\n",
    "\n",
    "# replace DC info to correspond to GeoCov19 format\n",
    "dc_loc = fips[fips['County or equivalent']=='DISTRICT OF COLUMBIA'].index.tolist()[0]\n",
    "fips['State or equivalent'].loc[dc_loc] = 'WASHINGTON, D.C.'\n",
    "fips['County or equivalent'].loc[dc_loc] = 'WASHINGTON'\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# create dictionary\n",
    "state_fips_dict = {k: f.groupby('County or equivalent')['FIPS'].apply(list).to_dict()\n",
    "     for k, f in fips.groupby('State or equivalent')}\n",
    "\n",
    "# clean dictionary\n",
    "for state in state_fips_dict:\n",
    "    for county in state_fips_dict[state]:\n",
    "        state_fips_dict[state][county] = state_fips_dict[state][county][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map county names to FIPS (using FIPS dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FIPS from a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fips_from_loc(loc):\n",
    "    '''\n",
    "    input = tweet_location from GeoCov19 data (single dictionary)\n",
    "    output = FIPS code corresponding to counties mentioned\n",
    "    '''\n",
    "    try:\n",
    "       \n",
    "        if loc['COUNTRY_CODE']=='US':\n",
    "\n",
    "            try:\n",
    "\n",
    "                if loc['COUNTY'].split(\" \")[-1] in ('COUNTY', 'PARISH'):\n",
    "\n",
    "                    return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] ]\n",
    "\n",
    "                elif 'COUNTY' in loc:\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] + \" \" + \"COUNTY\"]\n",
    "\n",
    "                    except Exception as e:\n",
    "\n",
    "                        pass\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                pass\n",
    "    except:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FIPS from a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fips_from_list_of_locs(tweet_locs):\n",
    "    '''\n",
    "    input = tweet_locations from GeoCov19 data (list of dicts)\n",
    "    output = list of FIPS codes corresponding to counties mentioned\n",
    "    '''\n",
    "\n",
    "    temp_list = [get_fips_from_loc(loc) for loc in tweet_locs]\n",
    "    \n",
    "    return [x for x in temp_list if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED TO INSPECT COUNTY MAPPINGS TO ENSURE THAT IT PICKS UP ON STRINGS WELL\n",
    "May have to implement fuzzy-matching if errors continue? Probably easiest/best to just find errors, since they won't change\n",
    "\n",
    "# ALL REPLACEMENTS MUST BE DONE IN \"fips\"\n",
    "* ~\"Pointe Coupee Parish County\" should be \"Pointe Coupee Parish\"~\n",
    "* ~Need to replace \"St.\" with \"Saint\" in \"fips\"~\n",
    "* Fix counties that aren't cross-referenced correctly:\n",
    "    * \"D.C.\"\n",
    "    * \"SAN FRANCISCO\"\n",
    "    * There are probably more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Tweet FIPS'] = df['tweet_locations'].apply(get_fips_from_list_of_locs)\n",
    "\n",
    "df['User FIPS'] = df['user_location'].apply(get_fips_from_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This looks unimportant but we need it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_from_list(my_list):\n",
    "\n",
    "    return [x for x in my_list if not pd.isnull(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of GeoCov19 Tweets with county-level user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_location   6.6260\n",
       "place           0.0708\n",
       "coordinates     0.0027\n",
       "Name: geo_source, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[~df['User FIPS'].isnull()]['geo_source'].value_counts() / len(df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_user(df):\n",
    "    '''\n",
    "    Input = GeoCov19 dataframe\n",
    "    Output = pd.DatFrame with four columns: 'user_id'; 'User FIPS - user_location', 'User FIPS - place', 'User FIPS - coordinates'\n",
    "    '''\n",
    "    \n",
    "    loc_type_dict = {}\n",
    "    \n",
    "    for loc_source in ['user_location', 'place', 'coordinates']:\n",
    "\n",
    "        loc_type_dict[loc_source] = df[ df['geo_source']==loc_source ].groupby(by='user_id').agg({'User FIPS': set})        \n",
    "    \n",
    "        loc_type_dict[loc_source]['User FIPS'] = loc_type_dict[loc_source]['User FIPS'].apply(list)\n",
    "    # merging\n",
    "    \n",
    "    grouped_df = loc_type_dict['user_location']\n",
    "    \n",
    "    grouped_df.columns = grouped_df.columns + ' - user_location'\n",
    "    \n",
    "    grouped_df = grouped_df.merge(loc_type_dict['place'], on='user_id', suffixes=(None, f\" - place\"), how='outer')\n",
    "    \n",
    "    grouped_df = grouped_df.merge(loc_type_dict['coordinates'], on='user_id', suffixes=(' - place', f\" - coordinates\"), how='outer')\n",
    "    \n",
    "    grouped_df.columns = str(df['Date'].unique()[0]) + ' - ' + grouped_df.columns\n",
    "    \n",
    "    return grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>2020-02-01 - User FIPS - user_location</th>\n",
       "      <th>2020-02-01 - User FIPS - place</th>\n",
       "      <th>2020-02-01 - User FIPS - coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13604612</td>\n",
       "      <td>[13121.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23328623</td>\n",
       "      <td>[42003.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60676618</td>\n",
       "      <td>[6037.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90670459</td>\n",
       "      <td>[48279.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171348971</td>\n",
       "      <td>[36029.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>228938650</td>\n",
       "      <td>[20091.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>322784463</td>\n",
       "      <td>[37119.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>359211782</td>\n",
       "      <td>[25025.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>593195091</td>\n",
       "      <td>[12099.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>893190151</td>\n",
       "      <td>[17097.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>987844879</td>\n",
       "      <td>[1007.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2674880963</td>\n",
       "      <td>[48113.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2776618730</td>\n",
       "      <td>[53033.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3290070751</td>\n",
       "      <td>[48201.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1118821379808419840</td>\n",
       "      <td>[6065.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1123938337130201088</td>\n",
       "      <td>[36067.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1137957898741403648</td>\n",
       "      <td>[48279.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1139720413880209408</td>\n",
       "      <td>[20157.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1222264839487225856</td>\n",
       "      <td>[6037.0]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60365817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[48113.0]</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user_id 2020-02-01 - User FIPS - user_location  \\\n",
       "0              13604612                              [13121.0]   \n",
       "1              23328623                              [42003.0]   \n",
       "2              60676618                               [6037.0]   \n",
       "3              90670459                              [48279.0]   \n",
       "4             171348971                              [36029.0]   \n",
       "5             228938650                              [20091.0]   \n",
       "6             322784463                              [37119.0]   \n",
       "7             359211782                              [25025.0]   \n",
       "8             593195091                              [12099.0]   \n",
       "9             893190151                              [17097.0]   \n",
       "10            987844879                               [1007.0]   \n",
       "11           2674880963                              [48113.0]   \n",
       "12           2776618730                              [53033.0]   \n",
       "13           3290070751                              [48201.0]   \n",
       "14  1118821379808419840                               [6065.0]   \n",
       "15  1123938337130201088                              [36067.0]   \n",
       "16  1137957898741403648                              [48279.0]   \n",
       "17  1139720413880209408                              [20157.0]   \n",
       "18  1222264839487225856                               [6037.0]   \n",
       "19             60365817                                    NaN   \n",
       "\n",
       "   2020-02-01 - User FIPS - place  2020-02-01 - User FIPS - coordinates  \n",
       "0                             NaN                                   nan  \n",
       "1                             NaN                                   nan  \n",
       "2                             NaN                                   nan  \n",
       "3                             NaN                                   nan  \n",
       "4                             NaN                                   nan  \n",
       "5                             NaN                                   nan  \n",
       "6                             NaN                                   nan  \n",
       "7                             NaN                                   nan  \n",
       "8                             NaN                                   nan  \n",
       "9                             NaN                                   nan  \n",
       "10                            NaN                                   nan  \n",
       "11                            NaN                                   nan  \n",
       "12                            NaN                                   nan  \n",
       "13                            NaN                                   nan  \n",
       "14                            NaN                                   nan  \n",
       "15                            NaN                                   nan  \n",
       "16                            NaN                                   nan  \n",
       "17                            NaN                                   nan  \n",
       "18                            NaN                                   nan  \n",
       "19                      [48113.0]                                   nan  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_by_user(df[~df['User FIPS'].isnull()].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL FUNCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* make FIPS df and dict\n",
    "* import master list of CT HT and Link users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_func(IMPORT_PATH, EXPORT_PATH):\n",
    "    \n",
    "    df = pd.read_json(fr\"{IMPORT_PATH}\", lines=True)\n",
    "    \n",
    "    # drop if not in master list of users\n",
    "    df = df.set_index('user_id').loc[MASTER_USER_LIST]\n",
    "    \n",
    "    # convert location strings to FIPS\n",
    "#     df['Tweet FIPS'] = df['tweet_locations'].apply(get_fips_from_list_of_locs)\n",
    "    df['User FIPS'] = df['user_location'].apply(get_fips_from_loc)\n",
    "    \n",
    "    # only keep obs with user_location\n",
    "    df = df[~df['User FIPS'].isnull()]\n",
    "    \n",
    "    # convert date to better format\n",
    "    df['Date'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d').dt.date\n",
    "    del df['created_at']\n",
    "    \n",
    "    # aggregate by user\n",
    "    df = aggregate_by_user(df)\n",
    "    \n",
    "    df.to_csv(fr\"{EXPORT_PATH_ROOT}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Final Function (Final Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-133-b16438c374ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mEXP_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEXPORT_PATH_ROOT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34mf\"{month_number}-{days}.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mfinal_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMP_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEXP_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-598ce8eed369>\u001b[0m in \u001b[0;36mfinal_func\u001b[1;34m(IMPORT_PATH, EXPORT_PATH)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinal_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMPORT_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEXPORT_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mfr\"{IMPORT_PATH}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tweet FIPS'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweet_locations'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_fips_from_list_of_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    751\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1119\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m             )\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "month_length_dict = {'feb':29, 'march':31, 'april':30}\n",
    "month_number_dict = {'feb':2, 'march':3, 'april':4}\n",
    "\n",
    "IMPORT_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/\"\n",
    "\n",
    "EXPORT_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/Intermediate Cleaned Data/\"\n",
    "\n",
    "for month in ['feb', 'march', 'april']:\n",
    "    \n",
    "    days = month_length_dict[month]\n",
    "    month_number = month_number_dict[month]\n",
    "    \n",
    "    for i in range(1, days + 1):\n",
    "        \n",
    "        if i < 10:\n",
    "            \n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_0{days}_10/geo_2020-{month_number}-0{days}/geo_2020-{month_number}-0{days}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"{month_number}-{days}.csv\"\n",
    "            \n",
    "            final_func(IMP_PATH, EXP_PATH)\n",
    "\n",
    "        elif i < 20:\n",
    "            \n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_{days}_20/geo_2020-{month_number}-{days}/geo_2020-{month_number}-{days}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"{month_number}-{days}.csv\"\n",
    "            \n",
    "            final_func(IMP_PATH, EXP_PATH)            \n",
    "        else:\n",
    "            \n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_{days}_30/geo_2020-{month_number}-{days}/geo_2020-{month_number}-{days}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"{month_number}-{days}.csv\"\n",
    "            \n",
    "            final_func(IMP_PATH, EXP_PATH)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to end up with each row being a user, and each col being the time period (week). \n",
    "\n",
    "(i,j)=[list of FIPS they tweeted from in this week]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
