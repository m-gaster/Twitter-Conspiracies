{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable codefolding/main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Identify US users \n",
    "2. Identify US Tweets \n",
    "3. Delete all obs. without US User or Tweet \n",
    "4. Match to list of users we collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GeoCov19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_json(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/geo_feb_01_10/geo_2020-02-01/geo_2020-02-01.json\", lines=True)\n",
    "\n",
    "# df = df.iloc[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_dict_uppercase(d):\n",
    "#     '''\n",
    "#     input = dictionary\n",
    "#     output = uppercase dictionary\n",
    "#     '''\n",
    "    \n",
    "#     return {key.upper(): value.upper() for key,value in d.items()}\n",
    "\n",
    "# df['user_location'] = df['user_location'].apply(make_dict_uppercase)\n",
    "\n",
    "\n",
    "# ##################################################################################\n",
    "\n",
    "\n",
    "# def make_list_of_dicts_uppercase(list_of_dicts):\n",
    "#     '''\n",
    "#     input = list of dictionaries\n",
    "#     output = uppercase list of dictionaries\n",
    "#     '''\n",
    "    \n",
    "#     return [make_dict_uppercase(d) for d in list_of_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tweet_locations'] = df['tweet_locations'].apply(make_list_of_dicts_uppercase)\n",
    "# df['user_location'] = df['user_location'].apply(make_dict_uppercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get (State & County) --> FIPS dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import data from wikipedia\n",
    "# fips = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\")[1]\n",
    "\n",
    "# # remove all hyperlinks (these look like \"... County [h]\", etc.)\n",
    "# fips['County or equivalent'] = fips['County or equivalent'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "\n",
    "# # convert to uppercase\n",
    "# fips['County or equivalent'] = fips['County or equivalent'].apply(lambda x: x.upper())\n",
    "# fips['State or equivalent'] = fips['State or equivalent'].apply(lambda x: x.upper())\n",
    "\n",
    "# # replace \"St.\" with \"Saint\"\n",
    "# fips['County or equivalent'] = [x.replace('ST.','SAINT') for x in fips['County or equivalent']]\n",
    "\n",
    "# # remove everything after a comma in a county name (e.g. \"ANCHORAGE, MUNICIPALITY OF\")\n",
    "# fips['County or equivalent'] = [x.split(',')[0] for x in fips['County or equivalent']]\n",
    "\n",
    "# # replace DC info to correspond to GeoCov19 format\n",
    "# dc_loc = fips[fips['County or equivalent']=='DISTRICT OF COLUMBIA'].index.tolist()[0]\n",
    "# fips['State or equivalent'].loc[dc_loc] = 'WASHINGTON, D.C.'\n",
    "# fips['County or equivalent'].loc[dc_loc] = 'WASHINGTON'\n",
    "\n",
    "# ########################################################################\n",
    "\n",
    "# # create dictionary\n",
    "# state_fips_dict = {k: f.groupby('County or equivalent')['FIPS'].apply(list).to_dict()\n",
    "#      for k, f in fips.groupby('State or equivalent')}\n",
    "\n",
    "# # clean dictionary\n",
    "# for state in state_fips_dict:\n",
    "#     for county in state_fips_dict[state]:\n",
    "#         state_fips_dict[state][county] = state_fips_dict[state][county][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map county names to FIPS (using FIPS dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FIPS from a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fips_from_loc(loc):\n",
    "#     '''\n",
    "#     input = tweet_location from GeoCov19 data (single dictionary)\n",
    "#     output = FIPS code corresponding to counties mentioned\n",
    "#     '''\n",
    "#     try:\n",
    "       \n",
    "#         if loc['COUNTRY_CODE']=='US':\n",
    "\n",
    "#             try:\n",
    "\n",
    "#                 if loc['COUNTY'].split(\" \")[-1] in ('COUNTY', 'PARISH'):\n",
    "\n",
    "#                     return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] ]\n",
    "\n",
    "#                 elif 'COUNTY' in loc:\n",
    "\n",
    "#                     try:\n",
    "\n",
    "#                         return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] + \" \" + \"COUNTY\"]\n",
    "\n",
    "#                     except Exception as e:\n",
    "\n",
    "#                         pass\n",
    "\n",
    "#             except Exception as e:\n",
    "\n",
    "#                 pass\n",
    "#     except:\n",
    "        \n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FIPS from a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fips_from_list_of_locs(tweet_locs):\n",
    "#     '''\n",
    "#     input = tweet_locations from GeoCov19 data (list of dicts)\n",
    "#     output = list of FIPS codes corresponding to counties mentioned\n",
    "#     '''\n",
    "\n",
    "#     temp_list = [get_fips_from_loc(loc) for loc in tweet_locs]\n",
    "    \n",
    "#     return [x for x in temp_list if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED TO INSPECT COUNTY MAPPINGS TO ENSURE THAT IT PICKS UP ON STRINGS WELL\n",
    "May have to implement fuzzy-matching if errors continue? Probably easiest/best to just find errors, since they won't change\n",
    "\n",
    "# ALL REPLACEMENTS MUST BE DONE IN \"fips\"\n",
    "* ~\"Pointe Coupee Parish County\" should be \"Pointe Coupee Parish\"~\n",
    "* ~Need to replace \"St.\" with \"Saint\" in \"fips\"~\n",
    "* Fix counties that aren't cross-referenced correctly:\n",
    "    * \"D.C.\"\n",
    "    * \"SAN FRANCISCO\"\n",
    "    * There are probably more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['Tweet FIPS'] = df['tweet_locations'].apply(get_fips_from_list_of_locs)\n",
    "\n",
    "# df['User FIPS'] = df['user_location'].apply(get_fips_from_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of GeoCov19 Tweets with county-level user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[~df['User FIPS'].isnull()]['geo_source'].value_counts() / len(df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aggregate_by_user(df):\n",
    "#     '''\n",
    "#     Input = GeoCov19 dataframe\n",
    "#     Output = pd.DatFrame with four columns: 'user_id'; 'User FIPS - user_location', 'User FIPS - place', 'User FIPS - coordinates'\n",
    "#     '''\n",
    "    \n",
    "#     loc_type_dict = {}\n",
    "    \n",
    "#     for loc_source in ['user_location', 'place', 'coordinates']:\n",
    "\n",
    "#         loc_type_dict[loc_source] = df[ df['geo_source']==loc_source ].groupby(by='user_id').agg({'User FIPS': set})        \n",
    "    \n",
    "#         loc_type_dict[loc_source]['User FIPS'] = loc_type_dict[loc_source]['User FIPS'].apply(list)\n",
    "#     # merging\n",
    "    \n",
    "#     grouped_df = loc_type_dict['user_location']\n",
    "    \n",
    "#     grouped_df.columns = grouped_df.columns + ' - user_location'\n",
    "    \n",
    "#     grouped_df = grouped_df.merge(loc_type_dict['place'], on='user_id', suffixes=(None, f\" - place\"), how='outer')\n",
    "    \n",
    "#     grouped_df = grouped_df.merge(loc_type_dict['coordinates'], on='user_id', suffixes=(' - place', f\" - coordinates\"), how='outer')\n",
    "        \n",
    "#     grouped_df.columns = str(df['Date'].unique()[0]) + ' - ' + grouped_df.columns\n",
    "    \n",
    "#     return grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Date'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate_by_user(df[~df['User FIPS'].isnull()].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL CODE STARTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_uppercase(d):\n",
    "    '''\n",
    "    input = dictionary\n",
    "    output = uppercase dictionary\n",
    "    '''\n",
    "    \n",
    "    return {key.upper(): value.upper() for key,value in d.items()}\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "def make_list_of_dicts_uppercase(list_of_dicts):\n",
    "    '''\n",
    "    input = list of dictionaries\n",
    "    output = uppercase list of dictionaries\n",
    "    '''\n",
    "    \n",
    "    return [make_dict_uppercase(d) for d in list_of_dicts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get (State & County) --> FIPS dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from wikipedia\n",
    "fips = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\")[1]\n",
    "\n",
    "# remove all hyperlinks (these look like \"... County [h]\", etc.)\n",
    "fips['County or equivalent'] = fips['County or equivalent'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "\n",
    "# convert to uppercase\n",
    "fips['County or equivalent'] = fips['County or equivalent'].apply(lambda x: x.upper())\n",
    "fips['State or equivalent'] = fips['State or equivalent'].apply(lambda x: x.upper())\n",
    "\n",
    "# replace \"St.\" with \"Saint\"\n",
    "fips['County or equivalent'] = [x.replace('ST.','SAINT') for x in fips['County or equivalent']]\n",
    "\n",
    "# remove everything after a comma in a county name (e.g. \"ANCHORAGE, MUNICIPALITY OF\")\n",
    "fips['County or equivalent'] = [x.split(',')[0] for x in fips['County or equivalent']]\n",
    "\n",
    "# replace DC info to correspond to GeoCov19 format\n",
    "dc_loc = fips[fips['County or equivalent']=='DISTRICT OF COLUMBIA'].index.tolist()[0]\n",
    "fips['State or equivalent'].loc[dc_loc] = 'WASHINGTON, D.C.'\n",
    "fips['County or equivalent'].loc[dc_loc] = 'WASHINGTON'\n",
    "\n",
    "########################################################################\n",
    "\n",
    "# create dictionary\n",
    "state_fips_dict = {k: f.groupby('County or equivalent')['FIPS'].apply(list).to_dict()\n",
    "     for k, f in fips.groupby('State or equivalent')}\n",
    "\n",
    "# clean dictionary\n",
    "for state in state_fips_dict:\n",
    "    for county in state_fips_dict[state]:\n",
    "        state_fips_dict[state][county] = state_fips_dict[state][county][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map county names to FIPS (using FIPS dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FIPS from a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fips_from_loc(loc):\n",
    "    '''\n",
    "    input = tweet_location from GeoCov19 data (single dictionary)\n",
    "    output = FIPS code corresponding to counties mentioned\n",
    "    '''\n",
    "    try:\n",
    "       \n",
    "        if loc['COUNTRY_CODE']=='US':\n",
    "\n",
    "            try:\n",
    "\n",
    "                if loc['COUNTY'].split(\" \")[-1] in ('COUNTY', 'PARISH'):\n",
    "\n",
    "                    return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] ]\n",
    "\n",
    "                elif 'COUNTY' in loc:\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] + \" \" + \"COUNTY\"]\n",
    "\n",
    "                    except Exception as e:\n",
    "\n",
    "                        pass\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                pass\n",
    "    except:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Locations to Individual Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_user(df):\n",
    "    '''\n",
    "    Input = GeoCov19 dataframe\n",
    "    Output = pd.DatFrame with four columns: 'user_id'; 'User FIPS - user_location', 'User FIPS - place', 'User FIPS - coordinates'\n",
    "    '''\n",
    "    \n",
    "    loc_type_dict = {}\n",
    "    \n",
    "    for loc_source in ['user_location', 'place', 'coordinates']:\n",
    "\n",
    "        loc_type_dict[loc_source] = df[ df['geo_source']==loc_source ].groupby(by='user_id').agg({'User FIPS': set})        \n",
    "    \n",
    "        loc_type_dict[loc_source]['User FIPS'] = loc_type_dict[loc_source]['User FIPS'].apply(list)\n",
    "    # merging\n",
    "    \n",
    "    grouped_df = loc_type_dict['user_location']\n",
    "    \n",
    "    grouped_df.columns = grouped_df.columns + ' - user_location'\n",
    "    \n",
    "    grouped_df = grouped_df.merge(loc_type_dict['place'], on='user_id', suffixes=(None, f\" - place\"), how='outer')\n",
    "    \n",
    "    grouped_df = grouped_df.merge(loc_type_dict['coordinates'], on='user_id', suffixes=(' - place', f\" - coordinates\"), how='outer')\n",
    "        \n",
    "    grouped_df.columns = str(df['Date'].unique()[0]) + ' - ' + grouped_df.columns\n",
    "    \n",
    "    return grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import All CT Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_USER_LIST = pd.read_csv(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/MASTER LIST - All CT Link and Hashtag Users/All CT Link and Hashtag Users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL FUNCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_func(IMPORT_PATH, EXPORT_PATH):\n",
    "    \n",
    "    df = pd.read_json(fr\"{IMPORT_PATH}\", lines=True)\n",
    "    \n",
    "    # drop if not in master list of users\n",
    "    user_intersection = np.intersect1d(df['user_id'].unique(), MASTER_USER_LIST['ID'].unique(), assume_unique=True)\n",
    "    df = df.set_index('user_id').loc[user_intersection]\n",
    "    \n",
    "    # make uppercase\n",
    "    df['user_location'] = df['user_location'].apply(make_dict_uppercase)\n",
    "    #df['tweet_locations'] = df['tweet_locations'].apply(make_list_of_dicts_uppercase)\n",
    "    \n",
    "    # convert location strings to FIPS\n",
    "    df['User FIPS'] = df['user_location'].apply(get_fips_from_loc)\n",
    "    #df['Tweet FIPS'] = df['tweet_locations'].apply(get_fips_from_list_of_locs)\n",
    "    \n",
    "    # only keep obs with user_location\n",
    "    df = df[~df['User FIPS'].isnull()]\n",
    "    \n",
    "    # convert date to better format\n",
    "    df['Date'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d').dt.date\n",
    "    del df['created_at']\n",
    "    \n",
    "    # aggregate by user\n",
    "    df = aggregate_by_user(df)\n",
    "            \n",
    "    df.to_csv(fr\"{EXPORT_PATH}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Final Function (Final Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_length_dict = {'feb':29, 'march':31, 'april':30}\n",
    "month_number_dict = {'feb':2, 'march':3, 'april':4}\n",
    "\n",
    "IMPORT_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/\"\n",
    "\n",
    "EXPORT_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/Intermediate Cleaned Data/\"\n",
    "\n",
    "for month in ['feb', 'march', 'april']:\n",
    "    \n",
    "    days = month_length_dict[month]\n",
    "    month_number = month_number_dict[month]\n",
    "    \n",
    "#     for i in range(1, days + 1):\n",
    "    for i in range(20, days + 1):\n",
    "        \n",
    "        if i < 10:\n",
    "            \n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_01_10/geo_2020-0{month_number}-0{i}/geo_2020-0{month_number}-0{i}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"2020-{month_number}-{i}.csv\"\n",
    "            \n",
    "            print(i, '\\n', IMP_PATH, '\\n', EXP_PATH)\n",
    "            final_func(IMP_PATH, EXP_PATH)\n",
    "            \n",
    "        elif i == 10:\n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_01_10/geo_2020-0{month_number}-{i}/geo_2020-0{month_number}-{i}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"2020-{month_number}-{i}.csv\"\n",
    "            \n",
    "            print(i, '\\n', IMP_PATH, '\\n', EXP_PATH)\n",
    "            final_func(IMP_PATH, EXP_PATH)\n",
    "\n",
    "        elif i <= 20:\n",
    "            \n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_11_20/geo_2020-0{month_number}-{i}/geo_2020-0{month_number}-{i}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"2020-{month_number}-{i}.csv\"\n",
    "            \n",
    "            print(i, '\\n', IMP_PATH, '\\n', EXP_PATH)\n",
    "            final_func(IMP_PATH, EXP_PATH)            \n",
    "        else:\n",
    "            \n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_21_{days}/geo_2020-0{month_number}-{i}/geo_2020-0{month_number}-{i}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"2020-{month_number}-{i}.csv\"\n",
    "            \n",
    "            print(i, '\\n', IMP_PATH, '\\n', EXP_PATH)\n",
    "            final_func(IMP_PATH, EXP_PATH)            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
