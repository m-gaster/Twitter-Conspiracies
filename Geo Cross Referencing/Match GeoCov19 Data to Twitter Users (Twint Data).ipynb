{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "1. Identify US users \n",
    "2. Identify US Tweets \n",
    "3. Delete all obs. without US User or Tweet \n",
    "4. Match to list of users we collected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GeoCov19 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_json(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/geo_feb_01_10/geo_2020-02-01/geo_2020-02-01.json\", lines=True)\n",
    "\n",
    "# df = np.array_split(df, num_chunks)[chunk_num]\n",
    "# df = df.iloc[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chunk in df:\n",
    "#     print(pd.read_json(chunk).head())\n",
    "\n",
    "# from io import StringIO\n",
    "\n",
    "# with pd.read_json(StringIO(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/geo_feb_01_10/geo_2020-02-01/geo_2020-02-01.json\"), lines=True, chunksize=100000) as reader:\n",
    "#     reader\n",
    "#     for chunk in reader:\n",
    "#         print(chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_dict_uppercase(d):\n",
    "#     '''\n",
    "#     input = dictionary\n",
    "#     output = uppercase dictionary\n",
    "#     '''\n",
    "    \n",
    "#     return {key.upper(): value.upper() for key,value in d.items()}\n",
    "\n",
    "# df['user_location'] = df['user_location'].apply(make_dict_uppercase)\n",
    "\n",
    "\n",
    "# ##################################################################################\n",
    "\n",
    "\n",
    "# def make_list_of_dicts_uppercase(list_of_dicts):\n",
    "#     '''\n",
    "#     input = list of dictionaries\n",
    "#     output = uppercase list of dictionaries\n",
    "#     '''\n",
    "    \n",
    "#     return [make_dict_uppercase(d) for d in list_of_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['tweet_locations'] = df['tweet_locations'].apply(make_list_of_dicts_uppercase)\n",
    "# df['user_location'] = df['user_location'].apply(make_dict_uppercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get (State & County) --> FIPS dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import data from wikipedia\n",
    "# fips = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\")[1]\n",
    "\n",
    "# # remove all hyperlinks (these look like \"... County [h]\", etc.)\n",
    "# fips['County or equivalent'] = fips['County or equivalent'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "\n",
    "# # convert to uppercase\n",
    "# fips['County or equivalent'] = fips['County or equivalent'].apply(lambda x: x.upper())\n",
    "# fips['State or equivalent'] = fips['State or equivalent'].apply(lambda x: x.upper())\n",
    "\n",
    "# # replace \"St.\" with \"Saint\"\n",
    "# fips['County or equivalent'] = [x.replace('ST.','SAINT') for x in fips['County or equivalent']]\n",
    "\n",
    "# # remove everything after a comma in a county name (e.g. \"ANCHORAGE, MUNICIPALITY OF\")\n",
    "# fips['County or equivalent'] = [x.split(',')[0] for x in fips['County or equivalent']]\n",
    "\n",
    "# # replace DC info to correspond to GeoCov19 format\n",
    "# dc_loc = fips[fips['County or equivalent']=='DISTRICT OF COLUMBIA'].index.tolist()[0]\n",
    "# fips['State or equivalent'].loc[dc_loc] = 'WASHINGTON, D.C.'\n",
    "# fips['County or equivalent'].loc[dc_loc] = 'WASHINGTON'\n",
    "\n",
    "# ########################################################################\n",
    "\n",
    "# # create dictionary\n",
    "# state_fips_dict = {k: f.groupby('County or equivalent')['FIPS'].apply(list).to_dict()\n",
    "#      for k, f in fips.groupby('State or equivalent')}\n",
    "\n",
    "# # clean dictionary\n",
    "# for state in state_fips_dict:\n",
    "#     for county in state_fips_dict[state]:\n",
    "#         state_fips_dict[state][county] = state_fips_dict[state][county][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map county names to FIPS (using FIPS dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FIPS from a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fips_from_loc(loc):\n",
    "#     '''\n",
    "#     input = tweet_location from GeoCov19 data (single dictionary)\n",
    "#     output = FIPS code corresponding to counties mentioned\n",
    "#     '''\n",
    "#     try:\n",
    "       \n",
    "#         if loc['COUNTRY_CODE']=='US':\n",
    "\n",
    "#             try:\n",
    "\n",
    "#                 if loc['COUNTY'].split(\" \")[-1] in ('COUNTY', 'PARISH'):\n",
    "\n",
    "#                     return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] ]\n",
    "\n",
    "#                 elif 'COUNTY' in loc:\n",
    "\n",
    "#                     try:\n",
    "\n",
    "#                         return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] + \" \" + \"COUNTY\"]\n",
    "\n",
    "#                     except Exception as e:\n",
    "\n",
    "#                         pass\n",
    "\n",
    "#             except Exception as e:\n",
    "\n",
    "#                 pass\n",
    "#     except:\n",
    "        \n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FIPS from a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fips_from_list_of_locs(tweet_locs):\n",
    "#     '''\n",
    "#     input = tweet_locations from GeoCov19 data (list of dicts)\n",
    "#     output = list of FIPS codes corresponding to counties mentioned\n",
    "#     '''\n",
    "\n",
    "#     temp_list = [get_fips_from_loc(loc) for loc in tweet_locs]\n",
    "    \n",
    "#     return [x for x in temp_list if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEED TO INSPECT COUNTY MAPPINGS TO ENSURE THAT IT PICKS UP ON STRINGS WELL\n",
    "May have to implement fuzzy-matching if errors continue? Probably easiest/best to just find errors, since they won't change\n",
    "\n",
    "# ALL REPLACEMENTS MUST BE DONE IN \"fips\"\n",
    "* ~\"Pointe Coupee Parish County\" should be \"Pointe Coupee Parish\"~\n",
    "* ~Need to replace \"St.\" with \"Saint\" in \"fips\"~\n",
    "* Fix counties that aren't cross-referenced correctly:\n",
    "    * \"D.C.\"\n",
    "    * \"SAN FRANCISCO\"\n",
    "    * There are probably more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['Tweet FIPS'] = df['tweet_locations'].apply(get_fips_from_list_of_locs)\n",
    "\n",
    "# df['User FIPS'] = df['user_location'].apply(get_fips_from_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentage of GeoCov19 Tweets with county-level user data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[~df['User FIPS'].isnull()]['geo_source'].value_counts() / len(df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aggregate_by_user(df):\n",
    "#     '''\n",
    "#     Input = GeoCov19 dataframe\n",
    "#     Output = pd.DatFrame with four columns: 'user_id'; 'User FIPS - user_location', 'User FIPS - place', 'User FIPS - coordinates'\n",
    "#     '''\n",
    "    \n",
    "#     loc_type_dict = {}\n",
    "    \n",
    "#     for loc_source in ['user_location', 'place', 'coordinates']:\n",
    "\n",
    "#         loc_type_dict[loc_source] = df[ df['geo_source']==loc_source ].groupby(by='user_id').agg({'User FIPS': set})        \n",
    "    \n",
    "#         loc_type_dict[loc_source]['User FIPS'] = loc_type_dict[loc_source]['User FIPS'].apply(list)\n",
    "#     # merging\n",
    "    \n",
    "#     grouped_df = loc_type_dict['user_location']\n",
    "    \n",
    "#     grouped_df.columns = grouped_df.columns + ' - user_location'\n",
    "    \n",
    "#     grouped_df = grouped_df.merge(loc_type_dict['place'], on='user_id', suffixes=(None, f\" - place\"), how='outer')\n",
    "    \n",
    "#     grouped_df = grouped_df.merge(loc_type_dict['coordinates'], on='user_id', suffixes=(' - place', f\" - coordinates\"), how='outer')\n",
    "        \n",
    "#     grouped_df.columns = str(df['Date'].unique()[0]) + ' - ' + grouped_df.columns\n",
    "    \n",
    "#     return grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Date'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate_by_user(df[~df['User FIPS'].isnull()].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL CODE STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 199)\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dict_uppercase(d):\n",
    "    '''\n",
    "    input = dictionary\n",
    "    output = uppercase dictionary\n",
    "    '''\n",
    "    \n",
    "    return {key.upper(): value.upper() for key,value in d.items()}\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "def make_list_of_dicts_uppercase(list_of_dicts):\n",
    "    '''\n",
    "    input = list of dictionaries\n",
    "    output = uppercase list of dictionaries\n",
    "    '''\n",
    "    \n",
    "    return [make_dict_uppercase(d) for d in list_of_dicts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get (State & County) --> FIPS dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# import data from wikipedia\n",
    "fips = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_FIPS_codes_by_county\")[1]\n",
    "\n",
    "# remove all hyperlinks (these look like \"... County [h]\", etc.)\n",
    "fips['County or equivalent'] = fips['County or equivalent'].str.replace(r\"\\[.*\\]\",\"\")\n",
    "\n",
    "# convert to uppercase\n",
    "fips['County or equivalent'] = fips['County or equivalent'].apply(lambda x: x.upper())\n",
    "fips['State or equivalent'] = fips['State or equivalent'].apply(lambda x: x.upper())\n",
    "\n",
    "# replace \"St.\" with \"Saint\"\n",
    "fips['County or equivalent'] = [x.replace('ST.','SAINT') for x in fips['County or equivalent']]\n",
    "\n",
    "# remove everything after a comma in a county name (e.g. \"ANCHORAGE, MUNICIPALITY OF\")\n",
    "fips['County or equivalent'] = [x.split(',')[0] for x in fips['County or equivalent']]\n",
    "\n",
    "# replace DC info to correspond to GeoCov19 format\n",
    "dc_loc = fips[fips['County or equivalent']=='DISTRICT OF COLUMBIA'].index.tolist()[0]\n",
    "fips['State or equivalent'].loc[dc_loc] = 'WASHINGTON, D.C.'\n",
    "fips['County or equivalent'].loc[dc_loc] = 'WASHINGTON'\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "# Save FIPS dataframe to file\n",
    "\n",
    "FIPS_SAVE_PATH = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/Geo Cross Referencing/County to FIPS.csv\"\n",
    "\n",
    "fips.to_csv(FIPS_SAVE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = pd.read_csv(FIPS_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create state:{county: fips} nested dictionary\n",
    "state_fips_dict = {k: f.groupby('County or equivalent')['FIPS'].apply(list).to_dict()\n",
    "     for k, f in fips.groupby('State or equivalent')}\n",
    "\n",
    "# clean dictionary\n",
    "for state in state_fips_dict:\n",
    "    for county in state_fips_dict[state]:\n",
    "        state_fips_dict[state][county] = state_fips_dict[state][county][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map county names to FIPS (using FIPS dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Matching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match_counties(county, counties_dict):\n",
    "    '''\n",
    "    input = county we're searching for; dict of counties we're trying to match it to and their correspodning FIPS\n",
    "    output = name of the county that it's closest to\n",
    "    '''\n",
    "    # create list of jaro-winkler similarities between the misspelled county and all counties in the state\n",
    "    jaro_distances = np.array([jellyfish.jaro_winkler_similarity(county, county_from_list) for county_from_list in list(counties_dict.keys())])\n",
    "        \n",
    "    # return county with smallest jaro-winkler distance\n",
    "    return counties_dict[list(counties_dict.keys())[np.argmin(jaro_distances)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FIPS from a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fips_from_loc(loc):\n",
    "    '''\n",
    "    input = tweet_location from US Election data: (STATE, COUNTY) tuple\n",
    "    output = FIPS code corresponding to county \n",
    "    '''\n",
    "    try:\n",
    "        if loc['COUNTRY_CODE']=='US':\n",
    "            try:\n",
    "                return state_fips_dict[ loc['STATE'] ][ loc['COUNTY'] ]\n",
    "            except:\n",
    "#                 print('not exact match')\n",
    "#                 print(loc['STATE'])\n",
    "#                 print(state_fips_dict[ loc['STATE'] ])\n",
    "                try:\n",
    "                    return fuzzy_match_counties(loc['COUNTY'], state_fips_dict[loc['STATE']])\n",
    "                except Exception as e:\n",
    "                    return np.nan\n",
    "        else:\n",
    "            return np.nan\n",
    "    except:\n",
    "#         print('not US')\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fips_from_loc(loc):\n",
    "#     '''\n",
    "#     input = tweet_location from GeoCov19 data (single dictionary)\n",
    "#     output = FIPS code corresponding to counties mentioned\n",
    "#     '''\n",
    "#     try:\n",
    "       \n",
    "#         if loc['COUNTRY_CODE']=='US':\n",
    "\n",
    "#             try:\n",
    "\n",
    "#                 if loc['COUNTY'].split(\" \")[-1] in ('COUNTY', 'PARISH'):\n",
    "\n",
    "#                     return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] ]\n",
    "\n",
    "#                 elif 'COUNTY' in loc:\n",
    "\n",
    "#                     try:\n",
    "\n",
    "#                         return state_fips_dict[ loc['STATE'] ] [loc['COUNTY'] + \" \" + \"COUNTY\"]\n",
    "\n",
    "#                     except Exception as e:\n",
    "\n",
    "#                         pass\n",
    "\n",
    "#             except Exception as e:\n",
    "\n",
    "#                 pass\n",
    "#     except:\n",
    "        \n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Locations to Individual Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_user(df):\n",
    "    '''\n",
    "    Input = GeoCov19 dataframe\n",
    "    Output = pd.DatFrame with four columns: 'user_id'; 'User FIPS - user_location', 'User FIPS - place', 'User FIPS - coordinates'\n",
    "    '''\n",
    "    \n",
    "    loc_type_dict = {}\n",
    "    \n",
    "    for loc_source in ['user_location', 'place', 'coordinates']:\n",
    "\n",
    "        loc_type_dict[loc_source] = df[ df['geo_source']==loc_source ].groupby(by='user_id').agg({'User FIPS': set})        \n",
    "    \n",
    "        loc_type_dict[loc_source]['User FIPS'] = loc_type_dict[loc_source]['User FIPS'].apply(list)\n",
    "    # merging\n",
    "    \n",
    "    grouped_df = loc_type_dict['user_location']\n",
    "    \n",
    "    grouped_df.columns = grouped_df.columns + ' - user_location'\n",
    "    \n",
    "    grouped_df = grouped_df.merge(loc_type_dict['place'], on='user_id', suffixes=(None, f\" - place\"), how='outer')\n",
    "    \n",
    "    grouped_df = grouped_df.merge(loc_type_dict['coordinates'], on='user_id', suffixes=(' - place', f\" - coordinates\"), how='outer')\n",
    "        \n",
    "    grouped_df.columns = str(df['Date'].unique()[0]) + ' - ' + grouped_df.columns\n",
    "    \n",
    "    return grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import All CT Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_USER_LIST = pd.read_csv(r\"C:/Users/crackcocaine69xxx/Python Stuff/594/Twint/MASTER LIST - All CT Link and Hashtag Users/All CT Link and Hashtag Users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL FUNCTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_func(IMPORT_PATH, EXPORT_PATH):#, num_chunks, chunk_num):\n",
    "    '''\n",
    "    num_chunks = number of chunks to split dataframe into\n",
    "    chunk_num = current chunk being worked on (should be iterating here).\n",
    "                chunk_num is in [0,...,num_chunks-1]\n",
    "    '''\n",
    "    \n",
    "    df = pd.read_json(fr\"{IMPORT_PATH}\", lines=True)\n",
    "    \n",
    "#     df = np.array_split(df, num_chunks)[chunk_num]\n",
    "    \n",
    "    # drop if not in master list of users\n",
    "    user_intersection = np.intersect1d(df['user_id'].unique(), MASTER_USER_LIST['ID'].unique(), assume_unique=True)\n",
    "    df = df.set_index('user_id').loc[user_intersection]\n",
    "    \n",
    "    # make uppercase\n",
    "    df['user_location'] = df['user_location'].apply(make_dict_uppercase)\n",
    "    #df['tweet_locations'] = df['tweet_locations'].apply(make_list_of_dicts_uppercase)\n",
    "    \n",
    "    # convert location strings to FIPS\n",
    "    df['User FIPS'] = df['user_location'].apply(get_fips_from_loc)\n",
    "    #df['Tweet FIPS'] = df['tweet_locations'].apply(get_fips_from_list_of_locs)\n",
    "    \n",
    "    # only keep obs with user_location\n",
    "    df = df[~df['User FIPS'].isnull()]\n",
    "    \n",
    "    # convert date to better format\n",
    "    df['Date'] = pd.to_datetime(df['created_at'], format='%Y-%m-%d').dt.date\n",
    "    del df['created_at']\n",
    "    \n",
    "    # aggregate by user\n",
    "    df = aggregate_by_user(df)\n",
    "            \n",
    "#     return df\n",
    "    df.to_csv(fr\"{EXPORT_PATH}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = final_func(IMPORT_PATH='C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/geo_feb_01_10/geo_2020-02-01/geo_2020-02-01.json',\n",
    "                 EXPORT_PATH=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Final Function (Final Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      " C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/geo_feb_01_10/geo_2020-02-01/geo_2020-02-01.json \n",
      " C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/Intermediate Cleaned Data/2020-2-1 - SPLIT 0 - TEST.csv\n",
      "2 \n",
      " C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/geo_feb_01_10/geo_2020-02-02/geo_2020-02-02.json \n",
      " C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/Intermediate Cleaned Data/2020-2-2 - SPLIT 0 - TEST.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-6df972b74b36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMP_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEXP_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mfinal_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIMP_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEXP_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, NUM_CHUNKS, chunk_num)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-831f41a23213>\u001b[0m in \u001b[0;36mfinal_func\u001b[1;34m(IMPORT_PATH, EXPORT_PATH)\u001b[0m\n\u001b[0;32m      6\u001b[0m     '''\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mfr\"{IMPORT_PATH}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#     df = np.array_split(df, num_chunks)[chunk_num]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m                 )\n\u001b[0;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 618\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    751\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    884\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    885\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 886\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1118\u001b[1;33m             self.obj = DataFrame(\n\u001b[0m\u001b[0;32m   1119\u001b[0m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m             )\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    507\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m                     \u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         return _list_of_dict_to_arrays(\n\u001b[0m\u001b[0;32m    527\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         )\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m_list_of_dict_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0msort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;31m# assure that they are of the base dict class and not of derived\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \"\"\"\n\u001b[0;32m    636\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m         \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m         \u001b[0msort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "month_length_dict = {'feb':29, 'march':31, 'april':30}\n",
    "month_number_dict = {'feb':2, 'march':3, 'april':4}\n",
    "\n",
    "processed_geocov_files = []\n",
    "\n",
    "# NUM_CHUNKS = 5\n",
    "\n",
    "IMPORT_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/\"\n",
    "\n",
    "EXPORT_PATH_ROOT = \"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/Intermediate Cleaned Data/\"\n",
    "\n",
    "for month in ['feb', 'march', 'april']:\n",
    "    \n",
    "    days = month_length_dict[month]\n",
    "    month_number = month_number_dict[month]\n",
    "    \n",
    "    for i in range(1, days + 1):\n",
    "#     for i in range(26, days + 1):\n",
    "        \n",
    "#         for chunk_num in range(0, NUM_CHUNKS):\n",
    "        \n",
    "        if i < 10:\n",
    "\n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_01_10/geo_2020-0{month_number}-0{i}/geo_2020-0{month_number}-0{i}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"2020-{month_number}-{i} - SPLIT {chunk_num} - TEST.csv\"\n",
    "            processed_geocov_files.append(EXP_PATH)\n",
    "\n",
    "            print(i, '\\n', IMP_PATH, '\\n', EXP_PATH)\n",
    "            final_func(IMP_PATH, EXP_PATH)#, NUM_CHUNKS, chunk_num)\n",
    "\n",
    "        elif i == 10:\n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_01_10/geo_2020-0{month_number}-{i}/geo_2020-0{month_number}-{i}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"2020-{month_number}-{i} - SPLIT {chunk_num}.csv\"\n",
    "            processed_geocov_files.append(EXP_PATH)\n",
    "\n",
    "            print(i, '\\n', IMP_PATH, '\\n', EXP_PATH)\n",
    "            final_func(IMP_PATH, EXP_PATH)# ,NUM_CHUNKS, chunk_num)\n",
    "\n",
    "        elif i <= 20:\n",
    "\n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_11_20/geo_2020-0{month_number}-{i}/geo_2020-0{month_number}-{i}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"2020-{month_number}-{i} - SPLIT {chunk_num}.csv\"\n",
    "            processed_geocov_files.append(EXP_PATH)\n",
    "\n",
    "            print(i, '\\n', IMP_PATH, '\\n', EXP_PATH)\n",
    "            final_func(IMP_PATH, EXP_PATH)# ,NUM_CHUNKS, chunk_num)            \n",
    "        else:\n",
    "\n",
    "            IMP_PATH = IMPORT_PATH_ROOT + fr\"geo_{month}_21_{days}/geo_2020-0{month_number}-{i}/geo_2020-0{month_number}-{i}.json\"\n",
    "            EXP_PATH = EXPORT_PATH_ROOT + f\"2020-{month_number}-{i} - SPLIT {chunk_num}.csv\"\n",
    "            processed_geocov_files.append(EXP_PATH)\n",
    "\n",
    "            print(i, '\\n', IMP_PATH, '\\n', EXP_PATH)\n",
    "            final_func(IMP_PATH, EXP_PATH)# ,NUM_CHUNKS, chunk_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Match to Twint Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-2-1.csv.csv',\n",
       " '2020-2-10.csv.csv',\n",
       " '2020-2-11.csv.csv',\n",
       " '2020-2-12.csv.csv',\n",
       " '2020-2-13.csv.csv',\n",
       " '2020-2-14.csv.csv',\n",
       " '2020-2-15.csv.csv',\n",
       " '2020-2-16.csv.csv',\n",
       " '2020-2-17.csv.csv',\n",
       " '2020-2-18.csv.csv',\n",
       " '2020-2-19.csv.csv',\n",
       " '2020-2-2.csv.csv',\n",
       " '2020-2-20.csv.csv',\n",
       " '2020-2-21.csv.csv',\n",
       " '2020-2-22.csv.csv',\n",
       " '2020-2-23.csv.csv',\n",
       " '2020-2-24.csv.csv',\n",
       " '2020-2-25.csv.csv',\n",
       " '2020-2-3.csv.csv',\n",
       " '2020-2-4.csv.csv',\n",
       " '2020-2-5.csv.csv',\n",
       " '2020-2-6.csv.csv',\n",
       " '2020-2-7.csv.csv',\n",
       " '2020-2-8.csv.csv',\n",
       " '2020-2-9.csv.csv']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:/Users/crackcocaine69xxx/Python Stuff/594/GeoCoV19 Data/Intermediate Cleaned Data/\"\n",
    "\n",
    "processed_geocov_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "df = pd.read_csv(path + processed_geocov_files.pop(0))\n",
    "\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "for file in processed_geocov_files:\n",
    "    \n",
    "    df = df.merge(pd.read_csv(path + file).drop(labels='Unnamed: 0', axis=1), how='outer', on='user_id')\n",
    "    \n",
    "# sort columns\n",
    "df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "# set index to 'user ID'\n",
    "df = df.set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-2-10.csv.csv',\n",
       " '2020-2-11.csv.csv',\n",
       " '2020-2-12.csv.csv',\n",
       " '2020-2-13.csv.csv',\n",
       " '2020-2-14.csv.csv',\n",
       " '2020-2-15.csv.csv',\n",
       " '2020-2-16.csv.csv',\n",
       " '2020-2-17.csv.csv',\n",
       " '2020-2-18.csv.csv',\n",
       " '2020-2-19.csv.csv',\n",
       " '2020-2-2.csv.csv',\n",
       " '2020-2-20.csv.csv',\n",
       " '2020-2-21.csv.csv',\n",
       " '2020-2-22.csv.csv',\n",
       " '2020-2-23.csv.csv',\n",
       " '2020-2-24.csv.csv',\n",
       " '2020-2-25.csv.csv',\n",
       " '2020-2-3.csv.csv',\n",
       " '2020-2-4.csv.csv',\n",
       " '2020-2-5.csv.csv',\n",
       " '2020-2-6.csv.csv',\n",
       " '2020-2-7.csv.csv',\n",
       " '2020-2-8.csv.csv',\n",
       " '2020-2-9.csv.csv']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(processed_geocov_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
